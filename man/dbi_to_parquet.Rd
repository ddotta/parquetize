% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/dbi_to_parquet.R
\name{dbi_to_parquet}
\alias{dbi_to_parquet}
\title{Convert a SQL Query on DBI connection to parquet format}
\usage{
dbi_to_parquet(
  dbi_connection,
  sql_query,
  path_to_parquet,
  parquetname,
  chunk_memory_size,
  chunk_size,
  sql_params = NULL,
  chunk_memory_sample_lines = 10000,
  partition = "no",
  ...
)
}
\arguments{
\item{dbi_connection}{DBI object (as return by a call to DBI::dbConnect) connection to the database}

\item{sql_query}{string the sql query used to get data, this argument is passed to DBI::dbSendQuery}

\item{path_to_parquet}{String that indicates the path to the directory where the parquet files will be stored.}

\item{parquetname}{string the base file name used. If not set, will use a contraction of the query.}

\item{chunk_memory_size}{Memory size (in Mb) in which data of one parquet file should roughly fit. For very small size, data could be a bit larger than given memory.}

\item{chunk_size}{Number of lines that defines the size of the chunk.
This argument must be filled in if `by_chunk` is TRUE (otherwise ignored).
This argument can not be filled in if chunk_memory_size is used.}

\item{sql_params}{list parameters to bind to sql query. This argument is passed to DBI::dbBind on the result of DBI::dbSendQuery}

\item{chunk_memory_sample_lines}{Number of lines to read to evaluate chunk_memory_size. Default to 10 000.}

\item{partition}{String ("yes" or "no" - by default) that indicates whether you want to create a partitioned parquet file.
If "yes", `"partitioning"` argument must be filled in. In this case, a folder will be created for each modality of the variable filled in `"partitioning"`.
Be careful, this argument can not be "yes" if `chunk_memory_size` or `chunk_size` argument are not NULL.}

\item{...}{additional format-specific arguments, see \href{https://arrow.apache.org/docs/r/reference/write_parquet.html}{arrow::write_parquet()}
and \href{https://arrow.apache.org/docs/r/reference/write_dataset.html}{arrow::write_dataset()} for more informations.}
}
\value{
A parquet file, invisibly
}
\description{
This function allows to convert a SQL query from a DBI to parquet format.\cr

It handles all DBI supported databases.

Two conversions possibilities are offered :

\itemize{

\item{Convert to a single parquet file. Argument `path_to_parquet` must then be used;}
\item{Convert to a partitioned parquet file. Additionnal arguments `partition` and `partitioning` must then be used;}

}

Examples explain how to convert a query to a chunked dataset
}
\examples{

# Conversion from a sqlite dbi connection to a single parquet file :

dbi_connection <- DBI::dbConnect(RSQLite::SQLite(),
  system.file("extdata","iris.sqlite",package = "parquetize"))

# Reading iris tavble from local sqlite database and
# and conversion to one parquet file :

dbi_to_parquet(
  dbi_connection = dbi_connection,
  sql_query = "SELECT * FROM iris",
  path_to_parquet = tempdir(),
  parquetname = "iris"
)

# Reading iris table from local sqlite database by chunk (using
# `chunk_memory_size` argument) and conversion to multiple parquet files

dbi_to_parquet(
  dbi_connection = dbi_connection,
  sql_query = "SELECT * FROM iris",
  path_to_parquet = tempdir(),
  parquetname = "iris",
  chunk_memory_size = 2 / 1024
)

# Using chunk and partition together is not possible directly but easy to do :
# Reading iris table from local sqlite database by chunk (using
# `chunk_memory_size` argument) and conversion to arrow dataset partitioned by
# species

partitions <- DBI::dbGetQuery(dbi_connection, "SELECT distinct(Species) FROM iris")

for (species in partitions[,1]) {
  dbi_to_parquet(
    dbi_connection = dbi_connection,
    sql_query = paste0("SELECT * FROM iris where Species = $species"),
    sql_params = list(species = c(species)),
    path_to_parquet = file.path(tempdir(), "iris", paste0("Species=", species)),
    chunk_memory_size = 2 / 1024,
    parquetname = "iris-"
  )
}

}
