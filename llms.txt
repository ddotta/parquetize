# ðŸ“¦ Package `parquetize` ![](reference/figures/hex_parquetize.png)

R package that allows to convert databases of different formats (csv,
SAS, SPSS, Stata, rds, sqlite, JSON, ndJSON) to
[parquet](https://parquet.apache.org/) format in a same function.

## Installation

To install `parquetize` from CRAN :

``` r
install.packages("parquetize")
```

Or alternatively to install the development version from GitHub :

``` r
remotes::install_github("ddotta/parquetize")
```

Then to load it :

``` r
library(parquetize)
```

## Why this package ?

This package is a simple wrapper of some very useful functions from the
[haven](https://github.com/tidyverse/haven),
[readr](https://github.com/tidyverse/readr/),
[jsonlite](https://github.com/jeroen/jsonlite),
[RSQLite](https://github.com/r-dbi/RSQLite) and
[arrow](https://github.com/apache/arrow) packages.

While working, I realized that I was often repeating the same operation
when working with parquet files :

- I import the file in R with {haven}, {jsonlite}, {readr}, {DBI} or
  {RSQLite}.
- And I export the file in parquet format

As a fervent of the DRY principle (donâ€™t repeat yourself) the exported
functions of this package make my life easier and **execute these
operations within the same function**.

**The last benefit** of using package
[parquetize](https://ddotta.github.io/parquetize/) is that its functions
allow to create single parquet files or partitioned files depending on
the arguments chosen in the functions.

- [csv_to_parquet()](https://ddotta.github.io/parquetize/reference/csv_to_parquet.html)
  - **The other benefit of this function** is that it allows you to
    convert csv or txt files whether they are stored locally or
    available on the internet directly to csv/txt format or inside a
    zip.
- [json_to_parquet()](https://ddotta.github.io/parquetize/reference/json_to_parquet.html)
  - **The other benefit of this function** is that it handles JSON and
    ndJSON files in a same function. There is only one function to use
    for these 2 cases.  
- [rds_to_parquet()](https://ddotta.github.io/parquetize/reference/rds_to_parquet.html)  
- [fst_to_parquet()](https://ddotta.github.io/parquetize/reference/fst_to_parquet.html)  
- [table_to_parquet()](https://ddotta.github.io/parquetize/reference/table_to_parquet.html)
  - **The other benefit of this function** is that it handles SAS, SPSS
    and Stata files in a same function. There is only one function to
    use for these 3 cases. To avoid overcharging Râ€™s RAM for huge table,
    the conversion can be done by chunk. For more information, see
    [here](https://ddotta.github.io/parquetize/articles/aa-conversions.html)
- [sqlite_to_parquet()](https://ddotta.github.io/parquetize/reference/sqlite_to_parquet.html)
- [dbi_to_parquet()](https://ddotta.github.io/parquetize/reference/dbi_to_parquet.html)

For more details, see the examples associated with each function in the
documentation.

## Example

You want to use the Insee file of first names by birth department? Use R
and {parquetize} package that takes care of everything: it downloads the
data (3.7 million rows) and converts it to parquet format in few seconds
!

![](reference/figures/Insee_example_csv.gif)

## Contribution

Feel welcome to contribute to add features that you find useful in your
daily work.  
Ideas are welcomed in [the
issues](https://github.com/ddotta/parquetize/issues).

# Package index

## Functions

The conversion functions available in this package

- [`csv_to_parquet()`](csv_to_parquet.md) : Convert a csv or a txt file
  to parquet format
- [`json_to_parquet()`](json_to_parquet.md) : Convert a json file to
  parquet format
- [`rds_to_parquet()`](rds_to_parquet.md) : Convert a rds file to
  parquet format
- [`fst_to_parquet()`](fst_to_parquet.md) : Convert a fst file to
  parquet format
- [`table_to_parquet()`](table_to_parquet.md) : Convert an input file to
  parquet format
- [`sqlite_to_parquet()`](sqlite_to_parquet.md) : Convert a sqlite file
  to parquet format
- [`dbi_to_parquet()`](dbi_to_parquet.md) : Convert a SQL Query on a DBI
  connection to parquet format

## Other functions

- [`get_parquet_info()`](get_parquet_info.md) : Get various info on
  parquet files
- [`get_partitions()`](get_partitions.md) : get unique values from
  table's column
- [`check_parquet()`](check_parquet.md) : Check if parquet file or
  dataset is readable and return basic informations
- [`download_extract()`](download_extract.md) : download and uncompress
  file if needed
- [`rbind_parquet()`](rbind_parquet.md) : Function to bind multiple
  parquet files by row
- [`parquetize_example()`](parquetize_example.md) : Get path to
  parquetize example

## Developers

- [`write_parquet_by_chunk()`](write_parquet_by_chunk.md) :

  read input by chunk on function and create dataset  

- [`write_parquet_at_once()`](write_parquet_at_once.md) :

  write parquet file or dataset based on partition argument  

# Articles

### All vignettes

- [Convert huge input file to parquet](aa-conversions.md):
