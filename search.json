[{"path":[]},{"path":"/articles/aa-conversions.html","id":"with-table_to_parquet","dir":"Articles","previous_headings":"Convert huge input file to parquet format","what":"With table_to_parquet()","title":"Convert huge input file to parquet","text":"huge input files SAS, SPSS Stata formats, parquetize package allows perform clever conversion using by_chunk=TRUE table_to_parquet() function. native behavior function (functions package) load entire table converted R write disk (single file partitioned directory). handling large files, risk frequently occurs R session aborts load entire database memory. risk even present work locally computer can limited work remote servers.table_to_parquet() offers solution answers need expressed parquetize users. idea split large table “chunks” based number rows table order able simultaneously : - read chunk large database - write chunk floor file Tip: number lines chunk must contain must supported RAM computer/server. Ideally, number chunks defined must limited. tens hundreds limit number intermediate files (see example ). example documentation using iris table. cut 150 rows 3 chunks 50 rows. example get 3 parquet files 50 lines called iris1-50.parquet, iris51-100.parquet iris101-151.parquet real life, can perform kind request parquetize API (example SAS file 50 000 000 lines defining 25 chunks 2 000 000 rows ) : Files myhugefile1-2000000.parquet, myhugefile2000001-4000000.parquet … created.","code":"table_to_parquet( path_to_table = system.file(\"examples\", \"iris.sas7bdat\", package = \"haven\"), path_to_parquet = tempfile(), by_chunk = TRUE, chunk_size = 50, encoding = \"utf-8\" ) table_to_parquet( path_to_table = \"myhugefile.sas7bdat\", path_to_parquet = tempdir(), by_chunk = TRUE, chunk_size = 2000000, encoding = \"utf-8\" )"},{"path":"/articles/aa-conversions.html","id":"function-rbind_parquet","dir":"Articles","previous_headings":"Convert huge input file to parquet format","what":"Function rbind_parquet()","title":"Convert huge input file to parquet","text":"end conversion table_to_parquet(), want reconstitute unique initial table computer resources (RAM) , can use helper function provided API rbind_parquet(). function allows bind multiple parquet files row. ’s example without deleting initial files (delete_initial_files=FALSE) : myhugefile.parquet file created myhugefile1-2000000.parquet, myhugefile2000001-4000000.parquet… files. :tada:","code":"rbind_parquet( folder = tempfile(), output_name = \"myhugefile\", delete_initial_files = FALSE )"},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Damien Dotta. Author, maintainer.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Dotta D (2023). parquetize: Convert Files Parquet Format. https://ddotta.github.io/parquetize/, https://github.com/ddotta/parquetize.","code":"@Manual{,   title = {parquetize: Convert Files to Parquet Format},   author = {Damien Dotta},   year = {2023},   note = {https://ddotta.github.io/parquetize/, https://github.com/ddotta/parquetize}, }"},{"path":"/index.html","id":"package-package-parquetize-","dir":"","previous_headings":"","what":"Convert Files to Parquet Format","title":"Convert Files to Parquet Format","text":"R package allows convert databases different formats (csv, SAS, SPSS, Stata, rds, sqlite, JSON, ndJSON) parquet format function.","code":""},{"path":"/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Convert Files to Parquet Format","text":"install parquetize CRAN : alternatively install development version GitHub : load :","code":"install.packages(\"parquetize\") remotes::install_github(\"ddotta/parquetize\") library(parquetize)"},{"path":"/index.html","id":"why-this-package-","dir":"","previous_headings":"","what":"Why this package ?","title":"Convert Files to Parquet Format","text":"package simple wrapper useful functions haven, readr, jsonlite, RSQLite arrow packages. working, realized often repeating operation working parquet files : import file R {haven}, {jsonlite}, {readr}, {DBI} {RSQLite}. export file parquet format fervent DRY principle (don’t repeat ) exported functions package make life easier execute operations within function. last benefit using package parquetize functions allow create single parquet files partitioned files depending arguments chosen functions. benefit function allows convert csv files whether stored locally available internet directly csv format inside zip. benefit function handles JSON ndJSON files function. one function use 2 cases. rds_to_parquet() benefit function handles SAS, SPSS Stata files function. one function use 3 cases. avoid overcharging R’s RAM huge table, conversion can done chunk. information, see sqlite_to_parquet() details, see documentation examples : - table_to_parquet(). - csv_to_parquet(). - json_to_parquet(). - rds_to_parquet(). - sqlite_to_parquet().","code":""},{"path":"/index.html","id":"example","dir":"","previous_headings":"","what":"Example","title":"Convert Files to Parquet Format","text":"want use Insee file first names birth department? Use R {parquetize} package takes care everything: downloads data (3.7 million rows) converts parquet format seconds !","code":""},{"path":"/index.html","id":"contribution","dir":"","previous_headings":"","what":"Contribution","title":"Convert Files to Parquet Format","text":"Feel welcome contribute add features find useful daily work. Ideas welcomed issues.","code":""},{"path":"/reference/csv_to_parquet.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert a csv file to parquet format — csv_to_parquet","title":"Convert a csv file to parquet format — csv_to_parquet","text":"function allows convert csv file parquet format. Several conversion possibilities offered : locally stored file. Argument `path_to_csv` must used; URL. Argument `url_to_csv` must used. Two conversions possibilities offered : Convert single parquet file. Argument `path_to_parquet` must used; Convert partitioned parquet file. Additionnal arguments `partition` `partitioning` must used;","code":""},{"path":"/reference/csv_to_parquet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert a csv file to parquet format — csv_to_parquet","text":"","code":"csv_to_parquet(   path_to_csv,   url_to_csv,   csv_as_a_zip = FALSE,   filename_in_zip,   path_to_parquet,   compression = \"snappy\",   compression_level = NULL,   partition = \"no\",   encoding = \"UTF-8\",   ... )"},{"path":"/reference/csv_to_parquet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert a csv file to parquet format — csv_to_parquet","text":"path_to_csv string indicates path csv file url_to_csv string indicates URL csv file csv_as_a_zip boolean indicates csv stored zip filename_in_zip name csv file zip (useful several csv included zip). Required `csv_as_a_zip` TRUE. path_to_parquet string indicates path directory parquet file stored compression compression algorithm. Default \"snappy\". compression_level compression level. Meaning depends compression algorithm. partition string (\"yes\" \"\" - default) indicates whether want create partitioned parquet file. \"yes\", `\"partitioning\"` argument must filled . case, folder created modality variable filled `\"partitioning\"`. encoding string indicates character encoding input file. ... additional format-specific arguments, see arrow::write_parquet() arrow::write_dataset() informations.","code":""},{"path":"/reference/csv_to_parquet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert a csv file to parquet format — csv_to_parquet","text":"parquet file, invisibly","code":""},{"path":"/reference/csv_to_parquet.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Convert a csv file to parquet format — csv_to_parquet","text":"careful, zip size exceeds 4 GB, function may truncate data (unzip() work reliably case - see ). case, advised unzip csv file hand (example 7-Zip) use function argument `path_to_csv`.","code":""},{"path":"/reference/csv_to_parquet.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert a csv file to parquet format — csv_to_parquet","text":"","code":"# Conversion from a local csv file to a single parquet file :  csv_to_parquet(   path_to_csv = parquetize_example(\"region_2022.csv\"),   path_to_parquet = tempdir() ) #> Reading data... #> Writing data... #> ✔ The csv file is available in parquet format under /tmp/RtmpuulHX9 #> Writing data...  # Conversion from a local csv file  to a partitioned parquet file  :  csv_to_parquet(   path_to_csv = parquetize_example(\"region_2022.csv\"),   path_to_parquet = tempdir(),   partition = \"yes\",   partitioning =  c(\"REG\") ) #> Reading data... #> Writing data... #> ✔ The csv file is available in parquet format under /tmp/RtmpuulHX9 #> Writing data...  # Conversion from a URL and a csv file with \"gzip\" compression :  csv_to_parquet(   url_to_csv =   \"https://github.com/sidsriv/Introduction-to-Data-Science-in-python/raw/master/census.csv\",   path_to_parquet = tempdir(),   compression = \"gzip\",   compression_level = 5 ) #> Reading data... #> ✔ The csv file is available in parquet format under /tmp/RtmpuulHX9 #> Reading data...  # Conversion from a URL and a zipped file :  csv_to_parquet(   url_to_csv = \"https://www.nomisweb.co.uk/output/census/2021/census2021-ts007.zip\",   csv_as_a_zip = TRUE,   filename_in_zip = \"census2021-ts007-ctry.csv\",   path_to_parquet = tempdir() ) #> Reading data... #> ✔ The csv file is available in parquet format under /tmp/RtmpuulHX9 #> Reading data..."},{"path":"/reference/json_to_parquet.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert a json file to parquet format — json_to_parquet","title":"Convert a json file to parquet format — json_to_parquet","text":"function allows convert json ndjson file parquet format. Two conversions possibilities offered : Convert single parquet file. Argument `path_to_parquet` must used; Convert partitioned parquet file. Additionnal arguments `partition` `partitioning` must used;","code":""},{"path":"/reference/json_to_parquet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert a json file to parquet format — json_to_parquet","text":"","code":"json_to_parquet(   path_to_json,   path_to_parquet,   format = \"json\",   partition = \"no\",   ... )"},{"path":"/reference/json_to_parquet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert a json file to parquet format — json_to_parquet","text":"path_to_json string indicates path csv file path_to_parquet string indicates path directory parquet file stored format string indicates format \"json\" (default) \"ndjson\" partition string (\"yes\" \"\" - default) indicates whether want create partitioned parquet file. \"yes\", `\"partitioning\"` argument must filled . case, folder created modality variable filled `\"partitioning\"`. ... additional format-specific arguments, see arrow::write_parquet() arrow::write_dataset() informations.","code":""},{"path":"/reference/json_to_parquet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert a json file to parquet format — json_to_parquet","text":"parquet file, invisibly","code":""},{"path":"/reference/json_to_parquet.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert a json file to parquet format — json_to_parquet","text":"","code":"# Conversion from a local json file to a single parquet file ::  json_to_parquet(   path_to_json = system.file(\"extdata\",\"iris.json\",package = \"parquetize\"),   path_to_parquet = tempdir() ) #> Reading data... #> Writing data... #> ✔ The json file is available in parquet format under /tmp/RtmpuulHX9 #> Writing data...  # Conversion from a local ndjson file to a partitioned parquet file  ::  json_to_parquet(   path_to_json = system.file(\"extdata\",\"iris.ndjson\",package = \"parquetize\"),   path_to_parquet = tempdir(),   format = \"ndjson\" ) #> Reading data... #> Writing data... #> ✔ The ndjson file is available in parquet format under /tmp/RtmpuulHX9 #> Writing data..."},{"path":"/reference/parquetize_example.html","id":null,"dir":"Reference","previous_headings":"","what":"Get path to parquetize example — parquetize_example","title":"Get path to parquetize example — parquetize_example","text":"parquetize comes bundled number sample files `inst/extdata` directory. function make easy access","code":""},{"path":"/reference/parquetize_example.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get path to parquetize example — parquetize_example","text":"","code":"parquetize_example(file = NULL)"},{"path":"/reference/parquetize_example.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get path to parquetize example — parquetize_example","text":"file Name file. `NULL`, example files listed.","code":""},{"path":"/reference/parquetize_example.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get path to parquetize example — parquetize_example","text":"character string","code":""},{"path":"/reference/parquetize_example.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get path to parquetize example — parquetize_example","text":"","code":"parquetize_example() #> [1] \"iris.duckdb\"     \"iris.json\"       \"iris.ndjson\"     \"iris.rds\"        #> [5] \"iris.sqlite\"     \"region_2022.csv\" parquetize_example(\"region_2022.csv\") #> [1] \"/home/runner/work/_temp/Library/parquetize/extdata/region_2022.csv\""},{"path":"/reference/rbind_parquet.html","id":null,"dir":"Reference","previous_headings":"","what":"Function to bind multiple parquet files by row — rbind_parquet","title":"Function to bind multiple parquet files by row — rbind_parquet","text":"function read parquet files `folder` argument starts `output_name`, combine using rbind write result new parquet file. can also delete initial files `delete_initial_files` argument TRUE. careful, function work files different structures present folder given argument `folder`.","code":""},{"path":"/reference/rbind_parquet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Function to bind multiple parquet files by row — rbind_parquet","text":"","code":"rbind_parquet(folder, output_name, delete_initial_files = TRUE)"},{"path":"/reference/rbind_parquet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Function to bind multiple parquet files by row — rbind_parquet","text":"folder folder initial files stored output_name name output parquet file delete_initial_files Boolean. function delete initial files ? default TRUE.","code":""},{"path":"/reference/rbind_parquet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Function to bind multiple parquet files by row — rbind_parquet","text":"Parquet files, invisibly","code":""},{"path":"/reference/rbind_parquet.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Function to bind multiple parquet files by row — rbind_parquet","text":"","code":"if (FALSE) { library(arrow) if (file.exists('output')==FALSE) {   dir.create(\"output\") }  file.create(fileext = \"output/test_data1-4.parquet\") write_parquet(data.frame(   x = c(\"a\",\"b\",\"c\"),   y = c(1L,2L,3L) ), \"output/test_data1-4.parquet\")  file.create(fileext = \"output/test_data4-6.parquet\") write_parquet(data.frame(   x = c(\"d\",\"e\",\"f\"),   y = c(4L,5L,6L) ), \"output/test_data4-6.parquet\")  test_data <- rbind_parquet(folder = \"output\",                            output_name = \"test_data\",                            delete_initial_files = FALSE) }"},{"path":"/reference/rds_to_parquet.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert a rds file to parquet format — rds_to_parquet","title":"Convert a rds file to parquet format — rds_to_parquet","text":"function allows convert rds file parquet format. Two conversions possibilities offered : Convert single parquet file. Argument `path_to_parquet` must used; Convert partitioned parquet file. Additionnal arguments `partition` `partitioning` must used;","code":""},{"path":"/reference/rds_to_parquet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert a rds file to parquet format — rds_to_parquet","text":"","code":"rds_to_parquet(path_to_rds, path_to_parquet, partition = \"no\", ...)"},{"path":"/reference/rds_to_parquet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert a rds file to parquet format — rds_to_parquet","text":"path_to_rds string indicates path rds file path_to_parquet string indicates path directory parquet file stored partition string (\"yes\" \"\" - default) indicates whether want create partitioned parquet file. \"yes\", `\"partitioning\"` argument must filled . case, folder created modality variable filled `\"partitioning\"`. ... additional format-specific arguments, see arrow::write_parquet() arrow::write_dataset() informations.","code":""},{"path":"/reference/rds_to_parquet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert a rds file to parquet format — rds_to_parquet","text":"parquet file, invisibly","code":""},{"path":"/reference/rds_to_parquet.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert a rds file to parquet format — rds_to_parquet","text":"","code":"# Conversion from a local rds file to a single parquet file ::  rds_to_parquet(   path_to_rds = system.file(\"extdata\",\"iris.rds\",package = \"parquetize\"),   path_to_parquet = tempdir() ) #> Reading data... #> Writing data... #> ✔ The rds file is available in parquet format under /tmp/RtmpuulHX9 #> Writing data...  # Conversion from a local rds file to a partitioned parquet file  ::  rds_to_parquet(   path_to_rds = system.file(\"extdata\",\"iris.rds\",package = \"parquetize\"),   path_to_parquet = tempdir(),   partition = \"yes\",   partitioning =  c(\"Species\") ) #> Reading data... #> Writing data... #> ✔ The rds file is available in parquet format under /tmp/RtmpuulHX9 #> Writing data..."},{"path":"/reference/sqlite_to_parquet.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert a sqlite file to parquet format — sqlite_to_parquet","title":"Convert a sqlite file to parquet format — sqlite_to_parquet","text":"function allows convert table sqlite file parquet format.  following extensions supported : \"db\",\"sdb\",\"sqlite\",\"db3\",\"s3db\",\"sqlite3\",\"sl3\",\"db2\",\"s2db\",\"sqlite2\",\"sl2\". Two conversions possibilities offered : Convert single parquet file. Argument `path_to_parquet` must used; Convert partitioned parquet file. Additionnal arguments `partition` `partitioning` must used;","code":""},{"path":"/reference/sqlite_to_parquet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert a sqlite file to parquet format — sqlite_to_parquet","text":"","code":"sqlite_to_parquet(   path_to_sqlite,   table_in_sqlite,   path_to_parquet,   partition = \"no\",   ... )"},{"path":"/reference/sqlite_to_parquet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert a sqlite file to parquet format — sqlite_to_parquet","text":"path_to_sqlite string indicates path sqlite file table_in_sqlite string indicates name table convert sqlite file path_to_parquet string indicates path directory parquet file stored partition string (\"yes\" \"\" - default) indicates whether want create partitioned parquet file. \"yes\", `\"partitioning\"` argument must filled . case, folder created modality variable filled `\"partitioning\"`. ... additional format-specific arguments, see arrow::write_parquet() arrow::write_dataset() informations.","code":""},{"path":"/reference/sqlite_to_parquet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert a sqlite file to parquet format — sqlite_to_parquet","text":"parquet file, invisibly","code":""},{"path":"/reference/sqlite_to_parquet.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert a sqlite file to parquet format — sqlite_to_parquet","text":"","code":"# Conversion from a local sqlite file to a single parquet file :  sqlite_to_parquet(   path_to_sqlite = system.file(\"extdata\",\"iris.sqlite\",package = \"parquetize\"),   table_in_sqlite = \"iris\",   path_to_parquet = tempdir() ) #> Reading data... #> Writing data... #> ✔ The iris table from your sqlite file is available in parquet format under /tmp/RtmpuulHX9 #> Writing data...  # Conversion from a local sqlite file to a partitioned parquet file  :  sqlite_to_parquet(   path_to_sqlite = system.file(\"extdata\",\"iris.sqlite\",package = \"parquetize\"),   table_in_sqlite = \"iris\",   path_to_parquet = tempdir(),   partition = \"yes\",   partitioning =  c(\"Species\") ) #> Reading data... #> Writing data... #> ✔ The iris table from your sqlite file is available in parquet format under /tmp/RtmpuulHX9 #> Writing data..."},{"path":"/reference/table_to_parquet.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert an input file to parquet format — table_to_parquet","title":"Convert an input file to parquet format — table_to_parquet","text":"function allows convert input file parquet format. handles SAS, SPSS Stata files function. one function use 3 cases. 3 cases, function guesses data format using extension input file (`path_to_table` argument). Two conversions possibilities offered : Convert single parquet file. Argument `path_to_parquet` must used; Convert partitioned parquet file. Additionnal arguments `partition` `partitioning` must used; avoid overcharging R's RAM, conversion can done chunk. Argument `by_chunk` must used. useful huge tables computers little RAM conversion done less memory consumption. information, see [](https://ddotta.github.io/parquetize/articles/aa-conversions.html).","code":""},{"path":"/reference/table_to_parquet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert an input file to parquet format — table_to_parquet","text":"","code":"table_to_parquet(   path_to_table,   path_to_parquet,   by_chunk = FALSE,   chunk_size,   skip = 0,   partition = \"no\",   encoding = NULL,   ... )"},{"path":"/reference/table_to_parquet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert an input file to parquet format — table_to_parquet","text":"path_to_table string indicates path input file (forget extension). path_to_parquet string indicates path directory parquet files stored. by_chunk Boolean. default FALSE. TRUE means conversion done chunk. chunk_size argument must filled `by_chunk` TRUE. Number lines defines size chunk. skip default 0. argument must filled `by_chunk` TRUE. Number lines ignore converting. partition string (\"yes\" \"\" - default) indicates whether want create partitioned parquet file. \"yes\", `\"partitioning\"` argument must filled . case, folder created modality variable filled `\"partitioning\"`. careful, `by_chunk` argument NULL single parquet file created. encoding string indicates character encoding input file. ... additional format-specific arguments,  see arrow::write_parquet() arrow::write_dataset() informations.","code":""},{"path":"/reference/table_to_parquet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert an input file to parquet format — table_to_parquet","text":"Parquet files, invisibly","code":""},{"path":"/reference/table_to_parquet.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert an input file to parquet format — table_to_parquet","text":"","code":"# Conversion from a SAS file to a single parquet file :  table_to_parquet(   path_to_table = system.file(\"examples\",\"iris.sas7bdat\", package = \"haven\"),   path_to_parquet = tempdir() ) #> Reading data... #> Writing data... #> ✔ The SAS file is available in parquet format under /tmp/RtmpuulHX9 #> Writing data...  # Conversion from a SPSS file to a single parquet file :  table_to_parquet(   path_to_table = system.file(\"examples\",\"iris.sav\", package = \"haven\"),   path_to_parquet = tempdir(), ) #> Reading data... #> Writing data... #> ✔ The SPSS file is available in parquet format under /tmp/RtmpuulHX9 #> Writing data... # Conversion from a Stata file to a single parquet file without progress bar :  table_to_parquet(   path_to_table = system.file(\"examples\",\"iris.dta\", package = \"haven\"),   path_to_parquet = tempdir() ) #> Reading data... #> Writing data... #> ✔ The Stata file is available in parquet format under /tmp/RtmpuulHX9 #> Writing data...  # Reading SAS file by chunk and with encoding and conversion # from a SAS file to a single parquet file :  table_to_parquet(   path_to_table = system.file(\"examples\",\"iris.sas7bdat\", package = \"haven\"),   path_to_parquet = tempdir(),   by_chunk = TRUE,   chunk_size = 50,   encoding = \"utf-8\" ) #> ✔ The SAS file is available in parquet format under /tmp/RtmpuulHX9/iris1-50.parquet #> ✔ The SAS file is available in parquet format under /tmp/RtmpuulHX9/iris51-100.parquet #> ✔ The SAS file is available in parquet format under /tmp/RtmpuulHX9/iris101-150.parquet  # Conversion from a SAS file to a partitioned parquet file  :  table_to_parquet(   path_to_table = system.file(\"examples\",\"iris.sas7bdat\", package = \"haven\"),   path_to_parquet = tempdir(),   partition = \"yes\",   partitioning =  c(\"Species\") # vector use as partition key ) #> Reading data... #> Writing data... #> ✔ The SAS file is available in parquet format under /tmp/RtmpuulHX9 #> Writing data..."},{"path":"/news/index.html","id":"parquetize-052","dir":"Changelog","previous_headings":"","what":"parquetize 0.5.2","title":"parquetize 0.5.2","text":"release fixes behaviour table_to_parquet() function argument by_chunk TRUE.","code":""},{"path":"/news/index.html","id":"parquetize-051","dir":"Changelog","previous_headings":"","what":"parquetize 0.5.1","title":"parquetize 0.5.1","text":"CRAN release: 2023-01-30 release removes duckdb_to_parquet() function advice Brian Ripley CRAN. Indeed, storage DuckDB yet stable. storage stabilized version 1.0 releases.","code":""},{"path":"/news/index.html","id":"parquetize-050","dir":"Changelog","previous_headings":"","what":"parquetize 0.5.0","title":"parquetize 0.5.0","text":"CRAN release: 2023-01-13 release includes corrections CRAN submission.","code":""},{"path":"/news/index.html","id":"parquetize-040","dir":"Changelog","previous_headings":"","what":"parquetize 0.4.0","title":"parquetize 0.4.0","text":"release includes important feature : table_to_parquet() function can now convert tables parquet format less memory consumption. Useful huge tables computers little RAM. (#15) vignette written . See . Removal nb_rows argument table_to_parquet() function Replaced new arguments by_chunk, chunk_size skip (see documentation) Progress bars now managed {cli} package","code":""},{"path":"/news/index.html","id":"parquetize-030","dir":"Changelog","previous_headings":"","what":"parquetize 0.3.0","title":"parquetize 0.3.0","text":"Added duckdb_to_parquet() function convert duckdb files parquet format. Added sqlite_to_parquet() function convert sqlite files parquet format.","code":""},{"path":"/news/index.html","id":"parquetize-020","dir":"Changelog","previous_headings":"","what":"parquetize 0.2.0","title":"parquetize 0.2.0","text":"Added rds_to_parquet() function convert rds files parquet format. Added json_to_parquet() function convert json ndjson files parquet format. Added possibility convert csv file partitioned parquet file. Improving code coverage (#9) Check path_to_parquet exists functions csv_to_parquet() table_to_parquet() (@py-b)","code":""},{"path":"/news/index.html","id":"parquetize-010","dir":"Changelog","previous_headings":"","what":"parquetize 0.1.0","title":"parquetize 0.1.0","text":"Added table_to_parquet() function convert SAS, SPSS Stata files parquet format. Added csv_to_parquet() function convert csv files parquet format. Added parquetize_example() function get path package data examples. Added NEWS.md file track changes package.","code":""}]
