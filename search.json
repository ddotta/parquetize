[{"path":"/CONTRIBUTING.html","id":null,"dir":"","previous_headings":"","what":"Contributing to parquetize development","title":"Contributing to parquetize development","text":"goal guide help get contributing parquetize quickly possible. guide divided two main pieces: Filing bug report feature request issue. Suggesting change via pull request.","code":""},{"path":"/CONTRIBUTING.html","id":"issues","dir":"","previous_headings":"","what":"Issues","title":"Contributing to parquetize development","text":"filing issue, important thing include minimal reproducible example can quickly verify problem, figure fix . three things need include make example reproducible: required packages, data, code. Packages loaded top script, ’s easy see ones example needs. easiest way include data use dput() generate R code recreate . example, recreate mtcars dataset R, ’d perform following steps: Run dput(mtcars) R Copy output reproducible script, type mtcars <- paste. even better can create data.frame() just handful rows columns still illustrates problem. Spend little bit time ensuring code easy others read: make sure ’ve used spaces variable names concise, informative use comments indicate problem lies best remove everything related problem. shorter code , easier understand. can check actually made reproducible example starting fresh R session pasting script . (Unless ’ve specifically asked , please don’t include output sessionInfo().)","code":""},{"path":"/CONTRIBUTING.html","id":"pull-requests","dir":"","previous_headings":"","what":"Pull requests","title":"Contributing to parquetize development","text":"contribute change parquetize, follow steps: Create branch git make changes. Push branch github issue pull request (PR). Discuss pull request. Iterate either accept PR decide ’s good fit parquetize. steps described detail . might feel overwhelming first time get set , gets easier practice. ’re familiar git github, please start reading http://r-pkgs..co.nz/git.html Pull requests evaluated seven point checklist: Motivation. pull request clearly concisely motivate need change. Also include motivation NEWS new release parquetize comes ’s easy users see ’s changed. Add item top file use markdown formatting. news item end (@yourGithubUsername, #the_issue_number). related changes. submit pull request, please check make sure haven’t accidentally included unrelated changes. make harder see exactly ’s changed, evaluate unexpected side effects. PR corresponds git branch, expect submit multiple changes make sure create multiple branches. multiple changes depend , start first one don’t submit others first one processed. ’re adding new parameters new function, ’ll also need document roxygen. Make sure re-run devtools::document() code submitting. fixing bug adding new feature, please add testthat unit test. seems like lot work don’t worry pull request isn’t perfect. pull request (“PR”) process, unless ’ve submitted past ’s unlikely pull request accepted . Many thanks advance !","code":""},{"path":[]},{"path":"/articles/aa-conversions.html","id":"with-table_to_parquet","dir":"Articles","previous_headings":"Convert huge input file to parquet format","what":"With table_to_parquet()","title":"Convert huge input file to parquet","text":"huge input files SAS, SPSS Stata formats, parquetize package allows perform clever conversion using chunk_memory_size chunk_size table_to_parquet() function. native behavior function (functions package) load entire table converted R write disk (single file partitioned directory). handling large files, risk frequently occurs R session aborts load entire database memory. risk even present work locally computer can limited work remote servers.table_to_parquet() offers solution answers need expressed parquetize users. examples documentation using iris table. ’s two ways split output files : memory consumption number lines","code":""},{"path":"/articles/aa-conversions.html","id":"spliting-data-by-memory-consumption","dir":"Articles","previous_headings":"Convert huge input file to parquet format > With table_to_parquet()","what":"Spliting data by memory consumption","title":"Convert huge input file to parquet","text":"table_to_parquet can guess number lines put file based memory consuption argument chunk_memory_size expressed Mb. cut 150 rows chunks roughly 5 Kb file loaded tibble. example get 2 parquet files 89 lines called iris1-89.parquet iris90-150.parquet real life, use chunk_memory_size Gb range, example SAS file 50 000 000 lines using chunk_memory_size 5000 Mb :","code":"table_to_parquet(   path_to_table = system.file(\"examples\", \"iris.sas7bdat\", package = \"haven\"),   path_to_parquet = tempfile(),   chunk_memory_size = 5 / 1024,   encoding = \"utf-8\" ) #> ✔ The SAS file is available in parquet format under /tmp/RtmpXLmyRq/file169711f40a0b/iris1-89.parquet #> ✔ The SAS file is available in parquet format under /tmp/RtmpXLmyRq/file169711f40a0b/iris90-150.parquet table_to_parquet(   path_to_table = \"myhugefile.sas7bdat\",   path_to_parquet = tempdir(),   chunk_memory_size = 5000,   encoding = \"utf-8\" )"},{"path":"/articles/aa-conversions.html","id":"splitting-data-by-number-of-lines","dir":"Articles","previous_headings":"Convert huge input file to parquet format > With table_to_parquet()","what":"Splitting data by number of lines","title":"Convert huge input file to parquet","text":"Tip: number lines chunk must contain must supported RAM computer/server. Ideally, number chunks defined must limited. tens hundreds limit number intermediate files (see example ). cut 150 rows 3 chunks 50 rows. example get 3 parquet files 50 lines called iris1-50.parquet, iris51-100.parquet iris101-151.parquet real life, can perform kind request parquetize API (example SAS file 50 000 000 lines defining 25 chunks 2 000 000 rows ) : Files myhugefile1-2000000.parquet, myhugefile2000001-4000000.parquet … created.","code":"table_to_parquet( path_to_table = system.file(\"examples\", \"iris.sas7bdat\", package = \"haven\"), path_to_parquet = tempfile(), chunk_size = 50, encoding = \"utf-8\" ) table_to_parquet( path_to_table = \"myhugefile.sas7bdat\", path_to_parquet = tempdir(), chunk_size = 2000000, encoding = \"utf-8\" )"},{"path":"/articles/aa-conversions.html","id":"function-rbind_parquet","dir":"Articles","previous_headings":"Convert huge input file to parquet format","what":"Function rbind_parquet()","title":"Convert huge input file to parquet","text":"end conversion table_to_parquet(), want reconstitute unique initial table computer resources (RAM) , can use helper function provided API rbind_parquet(). function allows bind multiple parquet files row. ’s example without deleting initial files (delete_initial_files=FALSE) : myhugefile.parquet file created myhugefile1-2000000.parquet, myhugefile2000001-4000000.parquet… files!","code":"rbind_parquet( folder = tempfile(), output_name = \"myhugefile\", delete_initial_files = FALSE )"},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Damien Dotta. Author, maintainer. Nicolas Chuche. Author.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Dotta D, Chuche N (2023). parquetize: Convert Files Parquet Format. https://ddotta.github.io/parquetize/, https://github.com/ddotta/parquetize.","code":"@Manual{,   title = {parquetize: Convert Files to Parquet Format},   author = {Damien Dotta and Nicolas Chuche},   year = {2023},   note = {https://ddotta.github.io/parquetize/, https://github.com/ddotta/parquetize}, }"},{"path":"/index.html","id":"package-package-parquetize-","dir":"","previous_headings":"","what":"Convert Files to Parquet Format","title":"Convert Files to Parquet Format","text":"R package allows convert databases different formats (csv, SAS, SPSS, Stata, rds, sqlite, JSON, ndJSON) parquet format function.","code":""},{"path":"/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Convert Files to Parquet Format","text":"install parquetize CRAN : alternatively install development version GitHub : load :","code":"install.packages(\"parquetize\") remotes::install_github(\"ddotta/parquetize\") library(parquetize)"},{"path":"/index.html","id":"why-this-package-","dir":"","previous_headings":"","what":"Why this package ?","title":"Convert Files to Parquet Format","text":"package simple wrapper useful functions haven, readr, jsonlite, RSQLite arrow packages. working, realized often repeating operation working parquet files : import file R {haven}, {jsonlite}, {readr}, {DBI} {RSQLite}. export file parquet format fervent DRY principle (don’t repeat ) exported functions package make life easier execute operations within function. last benefit using package parquetize functions allow create single parquet files partitioned files depending arguments chosen functions. benefit function allows convert csv files whether stored locally available internet directly csv format inside zip. benefit function handles JSON ndJSON files function. one function use 2 cases. rds_to_parquet() benefit function handles SAS, SPSS Stata files function. one function use 3 cases. avoid overcharging R’s RAM huge table, conversion can done chunk. information, see sqlite_to_parquet() details, see documentation examples : - table_to_parquet(). - csv_to_parquet(). - json_to_parquet(). - rds_to_parquet(). - sqlite_to_parquet().","code":""},{"path":"/index.html","id":"example","dir":"","previous_headings":"","what":"Example","title":"Convert Files to Parquet Format","text":"want use Insee file first names birth department? Use R {parquetize} package takes care everything: downloads data (3.7 million rows) converts parquet format seconds !","code":""},{"path":"/index.html","id":"contribution","dir":"","previous_headings":"","what":"Contribution","title":"Convert Files to Parquet Format","text":"Feel welcome contribute add features find useful daily work. Ideas welcomed issues.","code":""},{"path":"/reference/csv_to_parquet.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert a csv file to parquet format — csv_to_parquet","title":"Convert a csv file to parquet format — csv_to_parquet","text":"function allows convert csv file parquet format. Two conversions possibilities offered : Convert single parquet file. Argument `path_to_parquet` must used; Convert partitioned parquet file. Additionnal arguments `partition` `partitioning` must used;","code":""},{"path":"/reference/csv_to_parquet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert a csv file to parquet format — csv_to_parquet","text":"","code":"csv_to_parquet(   path_to_csv,   url_to_csv = deprecated(),   csv_as_a_zip = deprecated(),   filename_in_zip,   path_to_parquet,   columns = \"all\",   compression = \"snappy\",   compression_level = NULL,   partition = \"no\",   encoding = \"UTF-8\",   ... )"},{"path":"/reference/csv_to_parquet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert a csv file to parquet format — csv_to_parquet","text":"path_to_csv string indicates path url csv file url_to_csv DEPRECATED use path_to_csv instead csv_as_a_zip DEPRECATED filename_in_zip name csv file zip. Required several csv included zip. path_to_parquet string indicates path directory parquet file stored columns character vector columns select input file (default, columns selected). compression compression algorithm. Default \"snappy\". compression_level compression level. Meaning depends compression algorithm. partition string (\"yes\" \"\" - default) indicates whether want create partitioned parquet file. \"yes\", `\"partitioning\"` argument must filled . case, folder created modality variable filled `\"partitioning\"`. encoding string indicates character encoding input file. ... additional format-specific arguments, see arrow::write_parquet() arrow::write_dataset() informations.","code":""},{"path":"/reference/csv_to_parquet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert a csv file to parquet format — csv_to_parquet","text":"parquet file, invisibly","code":""},{"path":"/reference/csv_to_parquet.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Convert a csv file to parquet format — csv_to_parquet","text":"careful, zip size exceeds 4 GB, function may truncate data (unzip() work reliably case - see ). case, advised unzip csv file hand (example 7-Zip) use function argument `path_to_csv`.","code":""},{"path":"/reference/csv_to_parquet.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert a csv file to parquet format — csv_to_parquet","text":"","code":"# Conversion from a local csv file to a single parquet file :  csv_to_parquet(   path_to_csv = parquetize_example(\"region_2022.csv\"),   path_to_parquet = tempdir() ) #> Reading data... #> Writing data... #> ✔ The csv file is available in parquet format under /tmp/RtmpEZfIkY #> Writing data...  # Conversion from a local csv file to a single parquet file and select only # fex columns :  csv_to_parquet(   path_to_csv = parquetize_example(\"region_2022.csv\"),   path_to_parquet = tempdir(),   columns = c(\"REG\",\"LIBELLE\") ) #> Reading data... #> Writing data... #> ✔ The csv file is available in parquet format under /tmp/RtmpEZfIkY #> Writing data...  # Conversion from a local csv file  to a partitioned parquet file  :  csv_to_parquet(   path_to_csv = parquetize_example(\"region_2022.csv\"),   path_to_parquet = tempdir(),   partition = \"yes\",   partitioning =  c(\"REG\") ) #> Reading data... #> Writing data... #> ✔ The csv file is available in parquet format under /tmp/RtmpEZfIkY #> Writing data...  # Conversion from a URL and a csv file with \"gzip\" compression :  csv_to_parquet(   path_to_csv =   \"https://github.com/sidsriv/Introduction-to-Data-Science-in-python/raw/master/census.csv\",   path_to_parquet = tempdir(),   compression = \"gzip\",   compression_level = 5 ) #> Reading data... #> Writing data... #> ✔ The csv file is available in parquet format under /tmp/RtmpEZfIkY #> Writing data...  # Conversion from a URL and a zipped file :  csv_to_parquet(   path_to_csv = \"https://www.nomisweb.co.uk/output/census/2021/census2021-ts007.zip\",   filename_in_zip = \"census2021-ts007-ctry.csv\",   path_to_parquet = tempdir() ) #> Reading data... #> Writing data... #> ✔ The csv file is available in parquet format under /tmp/RtmpEZfIkY #> Writing data..."},{"path":"/reference/download_extract.html","id":null,"dir":"Reference","previous_headings":"","what":"download and uncompress file if needed — download_extract","title":"download and uncompress file if needed — download_extract","text":"function download file file remote   unzip zipped.  just return input path argument   neither. zip contains multiple files, can use `filename_in_zip` set file want unzip use. can pipe output `*_to_parquet` functions.","code":""},{"path":"/reference/download_extract.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"download and uncompress file if needed — download_extract","text":"","code":"download_extract(path, filename_in_zip)"},{"path":"/reference/download_extract.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"download and uncompress file if needed — download_extract","text":"path input  file's path url. filename_in_zip name csv file zip. Required several csv included zip.","code":""},{"path":"/reference/download_extract.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"download and uncompress file if needed — download_extract","text":"path usable (uncompressed) file, invisibly.","code":""},{"path":"/reference/download_extract.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"download and uncompress file if needed — download_extract","text":"","code":"# 1. unzip a local zip file # 2. parquetize it  file_path <- download_extract(system.file(\"extdata\",\"mtcars.csv.zip\", package = \"readr\")) csv_to_parquet(   file_path,   path_to_parquet = tempdir() ) #> Reading data... #> Writing data... #> ✔ The csv file is available in parquet format under /tmp/RtmpEZfIkY #> Writing data...  # 1. download a remote file # 2. extract the file census2021-ts007-ctry.csv # 3. parquetize it  file_path <- download_extract(   \"https://www.nomisweb.co.uk/output/census/2021/census2021-ts007.zip\",   filename_in_zip = \"census2021-ts007-ctry.csv\" ) csv_to_parquet(   file_path,   path_to_parquet = tempdir() ) #> Reading data... #> Writing data... #> ✔ The csv file is available in parquet format under /tmp/RtmpEZfIkY #> Writing data...  # the file is local and not zipped so : # 1. parquetize it  file_path <- download_extract(parquetize_example(\"region_2022.csv\")) csv_to_parquet(   file_path,   path_to_parquet = tempdir() ) #> Reading data... #> Writing data... #> ✔ The csv file is available in parquet format under /tmp/RtmpEZfIkY #> Writing data..."},{"path":"/reference/json_to_parquet.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert a json file to parquet format — json_to_parquet","title":"Convert a json file to parquet format — json_to_parquet","text":"function allows convert json ndjson file parquet format. Two conversions possibilities offered : Convert single parquet file. Argument `path_to_parquet` must used; Convert partitioned parquet file. Additionnal arguments `partition` `partitioning` must used;","code":""},{"path":"/reference/json_to_parquet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert a json file to parquet format — json_to_parquet","text":"","code":"json_to_parquet(   path_to_json,   path_to_parquet,   format = \"json\",   partition = \"no\",   ... )"},{"path":"/reference/json_to_parquet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert a json file to parquet format — json_to_parquet","text":"path_to_json string indicates path csv file path_to_parquet string indicates path directory parquet file stored format string indicates format \"json\" (default) \"ndjson\" partition string (\"yes\" \"\" - default) indicates whether want create partitioned parquet file. \"yes\", `\"partitioning\"` argument must filled . case, folder created modality variable filled `\"partitioning\"`. ... additional format-specific arguments, see arrow::write_parquet() arrow::write_dataset() informations.","code":""},{"path":"/reference/json_to_parquet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert a json file to parquet format — json_to_parquet","text":"parquet file, invisibly","code":""},{"path":"/reference/json_to_parquet.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert a json file to parquet format — json_to_parquet","text":"","code":"# Conversion from a local json file to a single parquet file ::  json_to_parquet(   path_to_json = system.file(\"extdata\",\"iris.json\",package = \"parquetize\"),   path_to_parquet = tempdir() ) #> Reading data... #> Writing data... #> ✔ The json file is available in parquet format under /tmp/RtmpEZfIkY #> Writing data...  # Conversion from a local ndjson file to a partitioned parquet file  ::  json_to_parquet(   path_to_json = system.file(\"extdata\",\"iris.ndjson\",package = \"parquetize\"),   path_to_parquet = tempdir(),   format = \"ndjson\" ) #> Reading data... #> Writing data... #> ✔ The ndjson file is available in parquet format under /tmp/RtmpEZfIkY #> Writing data..."},{"path":"/reference/parquetize_example.html","id":null,"dir":"Reference","previous_headings":"","what":"Get path to parquetize example — parquetize_example","title":"Get path to parquetize example — parquetize_example","text":"parquetize comes bundled number sample files `inst/extdata` directory. function make easy access","code":""},{"path":"/reference/parquetize_example.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get path to parquetize example — parquetize_example","text":"","code":"parquetize_example(file = NULL)"},{"path":"/reference/parquetize_example.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get path to parquetize example — parquetize_example","text":"file Name file. `NULL`, example files listed.","code":""},{"path":"/reference/parquetize_example.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get path to parquetize example — parquetize_example","text":"character string","code":""},{"path":"/reference/parquetize_example.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get path to parquetize example — parquetize_example","text":"","code":"parquetize_example() #>  [1] \"Species=setosa\"     \"Species=versicolor\" \"Species=virginica\"  #>  [4] \"iris.duckdb\"        \"iris.json\"          \"iris.ndjson\"        #>  [7] \"iris.parquet\"       \"iris.rds\"           \"iris.sqlite\"        #> [10] \"region_2022.csv\"    parquetize_example(\"region_2022.csv\") #> [1] \"/home/runner/work/_temp/Library/parquetize/extdata/region_2022.csv\""},{"path":"/reference/rbind_parquet.html","id":null,"dir":"Reference","previous_headings":"","what":"Function to bind multiple parquet files by row — rbind_parquet","title":"Function to bind multiple parquet files by row — rbind_parquet","text":"function read parquet files `folder` argument starts `output_name`, combine using rbind write result new parquet file. can also delete initial files `delete_initial_files` argument TRUE. careful, function work files different structures present folder given argument `folder`.","code":""},{"path":"/reference/rbind_parquet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Function to bind multiple parquet files by row — rbind_parquet","text":"","code":"rbind_parquet(folder, output_name, delete_initial_files = TRUE)"},{"path":"/reference/rbind_parquet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Function to bind multiple parquet files by row — rbind_parquet","text":"folder folder initial files stored output_name name output parquet file delete_initial_files Boolean. function delete initial files ? default TRUE.","code":""},{"path":"/reference/rbind_parquet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Function to bind multiple parquet files by row — rbind_parquet","text":"Parquet files, invisibly","code":""},{"path":"/reference/rbind_parquet.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Function to bind multiple parquet files by row — rbind_parquet","text":"","code":"if (FALSE) { library(arrow) if (file.exists('output')==FALSE) {   dir.create(\"output\") }  file.create(fileext = \"output/test_data1-4.parquet\") write_parquet(data.frame(   x = c(\"a\",\"b\",\"c\"),   y = c(1L,2L,3L) ), \"output/test_data1-4.parquet\")  file.create(fileext = \"output/test_data4-6.parquet\") write_parquet(data.frame(   x = c(\"d\",\"e\",\"f\"),   y = c(4L,5L,6L) ), \"output/test_data4-6.parquet\")  test_data <- rbind_parquet(folder = \"output\",                            output_name = \"test_data\",                            delete_initial_files = FALSE) }"},{"path":"/reference/rds_to_parquet.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert a rds file to parquet format — rds_to_parquet","title":"Convert a rds file to parquet format — rds_to_parquet","text":"function allows convert rds file parquet format. Two conversions possibilities offered : Convert single parquet file. Argument `path_to_parquet` must used; Convert partitioned parquet file. Additionnal arguments `partition` `partitioning` must used;","code":""},{"path":"/reference/rds_to_parquet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert a rds file to parquet format — rds_to_parquet","text":"","code":"rds_to_parquet(path_to_rds, path_to_parquet, partition = \"no\", ...)"},{"path":"/reference/rds_to_parquet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert a rds file to parquet format — rds_to_parquet","text":"path_to_rds string indicates path rds file path_to_parquet string indicates path directory parquet file stored partition string (\"yes\" \"\" - default) indicates whether want create partitioned parquet file. \"yes\", `\"partitioning\"` argument must filled . case, folder created modality variable filled `\"partitioning\"`. ... additional format-specific arguments, see arrow::write_parquet() arrow::write_dataset() informations.","code":""},{"path":"/reference/rds_to_parquet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert a rds file to parquet format — rds_to_parquet","text":"parquet file, invisibly","code":""},{"path":"/reference/rds_to_parquet.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert a rds file to parquet format — rds_to_parquet","text":"","code":"# Conversion from a local rds file to a single parquet file ::  rds_to_parquet(   path_to_rds = system.file(\"extdata\",\"iris.rds\",package = \"parquetize\"),   path_to_parquet = tempdir() ) #> Reading data... #> Writing data... #> ✔ The rds file is available in parquet format under /tmp/RtmpEZfIkY #> Writing data...  # Conversion from a local rds file to a partitioned parquet file  ::  rds_to_parquet(   path_to_rds = system.file(\"extdata\",\"iris.rds\",package = \"parquetize\"),   path_to_parquet = tempdir(),   partition = \"yes\",   partitioning =  c(\"Species\") ) #> Reading data... #> Writing data... #> ✔ The rds file is available in parquet format under /tmp/RtmpEZfIkY #> Writing data..."},{"path":"/reference/sqlite_to_parquet.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert a sqlite file to parquet format — sqlite_to_parquet","title":"Convert a sqlite file to parquet format — sqlite_to_parquet","text":"function allows convert table sqlite file parquet format.  following extensions supported : \"db\",\"sdb\",\"sqlite\",\"db3\",\"s3db\",\"sqlite3\",\"sl3\",\"db2\",\"s2db\",\"sqlite2\",\"sl2\". Two conversions possibilities offered : Convert single parquet file. Argument `path_to_parquet` must used; Convert partitioned parquet file. Additionnal arguments `partition` `partitioning` must used;","code":""},{"path":"/reference/sqlite_to_parquet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert a sqlite file to parquet format — sqlite_to_parquet","text":"","code":"sqlite_to_parquet(   path_to_sqlite,   table_in_sqlite,   path_to_parquet,   partition = \"no\",   ... )"},{"path":"/reference/sqlite_to_parquet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert a sqlite file to parquet format — sqlite_to_parquet","text":"path_to_sqlite string indicates path sqlite file table_in_sqlite string indicates name table convert sqlite file path_to_parquet string indicates path directory parquet file stored partition string (\"yes\" \"\" - default) indicates whether want create partitioned parquet file. \"yes\", `\"partitioning\"` argument must filled . case, folder created modality variable filled `\"partitioning\"`. ... additional format-specific arguments, see arrow::write_parquet() arrow::write_dataset() informations.","code":""},{"path":"/reference/sqlite_to_parquet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert a sqlite file to parquet format — sqlite_to_parquet","text":"parquet file, invisibly","code":""},{"path":"/reference/sqlite_to_parquet.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert a sqlite file to parquet format — sqlite_to_parquet","text":"","code":"# Conversion from a local sqlite file to a single parquet file :  sqlite_to_parquet(   path_to_sqlite = system.file(\"extdata\",\"iris.sqlite\",package = \"parquetize\"),   table_in_sqlite = \"iris\",   path_to_parquet = tempdir() ) #> Reading data... #> Writing data... #> ✔ The iris table from your sqlite file is available in parquet format under /tmp/RtmpEZfIkY #> Writing data...  # Conversion from a local sqlite file to a partitioned parquet file  :  sqlite_to_parquet(   path_to_sqlite = system.file(\"extdata\",\"iris.sqlite\",package = \"parquetize\"),   table_in_sqlite = \"iris\",   path_to_parquet = tempdir(),   partition = \"yes\",   partitioning =  c(\"Species\") ) #> Reading data... #> Writing data... #> ✔ The iris table from your sqlite file is available in parquet format under /tmp/RtmpEZfIkY #> Writing data..."},{"path":"/reference/table_to_parquet.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert an input file to parquet format — table_to_parquet","title":"Convert an input file to parquet format — table_to_parquet","text":"function allows convert input file parquet format. handles SAS, SPSS Stata files function. one function use 3 cases. 3 cases, function guesses data format using extension input file (`path_to_table` argument). Two conversions possibilities offered : Convert single parquet file. Argument `path_to_parquet` must used; Convert partitioned parquet file. Additionnal arguments `partition` `partitioning` must used; avoid overcharging R's RAM, conversion can done chunk. One arguments `chunk_memory_size` `chunk_size` must used. useful huge tables computers little RAM conversion done less memory consumption. information, see [](https://ddotta.github.io/parquetize/articles/aa-conversions.html).","code":""},{"path":"/reference/table_to_parquet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert an input file to parquet format — table_to_parquet","text":"","code":"table_to_parquet(   path_to_table,   path_to_parquet,   chunk_memory_size,   chunk_size,   columns = \"all\",   by_chunk = deprecated(),   skip = 0,   partition = \"no\",   encoding = NULL,   chunk_memory_sample_lines = 10000,   ... )"},{"path":"/reference/table_to_parquet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert an input file to parquet format — table_to_parquet","text":"path_to_table String indicates path input file (forget extension). path_to_parquet String indicates path directory parquet files stored. chunk_memory_size Memory size (Mb) data one parquet file roughly fit. small size, data bit larger given memory. chunk_size Number lines defines size chunk. argument must filled `by_chunk` TRUE (otherwise ignored). argument can filled chunk_memory_size used. columns Character vector columns select input file (default, columns selected). by_chunk DEPRECATED use chunk_memory_size chunk_memory instead skip default 0. argument must filled `by_chunk` TRUE. Number lines ignore converting. partition String (\"yes\" \"\" - default) indicates whether want create partitioned parquet file. \"yes\", `\"partitioning\"` argument must filled . case, folder created modality variable filled `\"partitioning\"`. careful, argument can \"yes\" `chunk_memory_size` `chunk_size` argument NULL. encoding String indicates character encoding input file. chunk_memory_sample_lines Number lines read evaluate chunk_memory_size. Default 10 000. ... Additional format-specific arguments,  see arrow::write_parquet() arrow::write_dataset() informations.","code":""},{"path":"/reference/table_to_parquet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert an input file to parquet format — table_to_parquet","text":"Parquet files, invisibly","code":""},{"path":"/reference/table_to_parquet.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert an input file to parquet format — table_to_parquet","text":"","code":"# Conversion from a SAS file to a single parquet file :  table_to_parquet(   path_to_table = system.file(\"examples\",\"iris.sas7bdat\", package = \"haven\"),   path_to_parquet = tempdir() ) #> Reading data... #> Writing data... #> ✔ The SAS file is available in parquet format under /tmp/RtmpEZfIkY #> Writing data...  # Conversion from a SPSS file to a single parquet file :  table_to_parquet(   path_to_table = system.file(\"examples\",\"iris.sav\", package = \"haven\"),   path_to_parquet = tempdir(), ) #> Reading data... #> Writing data... #> ✔ The SPSS file is available in parquet format under /tmp/RtmpEZfIkY #> Writing data... # Conversion from a Stata file to a single parquet file without progress bar :  table_to_parquet(   path_to_table = system.file(\"examples\",\"iris.dta\", package = \"haven\"),   path_to_parquet = tempdir() ) #> Reading data... #> Writing data... #> ✔ The Stata file is available in parquet format under /tmp/RtmpEZfIkY #> Writing data...  # Reading SPSS file by chunk (using `chunk_size` argument) # and conversion to multiple parquet files :  table_to_parquet(   path_to_table = system.file(\"examples\",\"iris.sav\", package = \"haven\"),   path_to_parquet = tempdir(),   chunk_size = 50, ) #> ✔ The SPSS file is available in parquet format under /tmp/RtmpEZfIkY/iris1-50.parquet #> ✔ The SPSS file is available in parquet format under /tmp/RtmpEZfIkY/iris51-100.parquet #> ✔ The SPSS file is available in parquet format under /tmp/RtmpEZfIkY/iris101-150.parquet  # Reading SPSS file by chunk (using `chunk_memory_size` argument) # and conversion to multiple parquet files of 5 Kb when loaded (5 Mb / 1024) # (in real files, you should use bigger value that fit in memory like 3000 # or 4000) :  table_to_parquet(   path_to_table = system.file(\"examples\",\"iris.sav\", package = \"haven\"),   path_to_parquet = tempdir(),   chunk_memory_size = 5 / 1024, ) #> ✔ The SPSS file is available in parquet format under /tmp/RtmpEZfIkY/iris1-82.parquet #> ✔ The SPSS file is available in parquet format under /tmp/RtmpEZfIkY/iris83-150.parquet  # Reading SAS file by chunk of 50 lines with encoding # and conversion to multiple files :  table_to_parquet(   path_to_table = system.file(\"examples\",\"iris.sas7bdat\", package = \"haven\"),   path_to_parquet = tempdir(),   chunk_size = 50,   encoding = \"utf-8\" ) #> ✔ The SAS file is available in parquet format under /tmp/RtmpEZfIkY/iris1-50.parquet #> ✔ The SAS file is available in parquet format under /tmp/RtmpEZfIkY/iris51-100.parquet #> ✔ The SAS file is available in parquet format under /tmp/RtmpEZfIkY/iris101-150.parquet  # Conversion from a SAS file to a single parquet file and select only # few columns  :  table_to_parquet(   path_to_table = system.file(\"examples\",\"iris.sas7bdat\", package = \"haven\"),   path_to_parquet = tempdir(),   columns = c(\"Species\",\"Petal_Length\") ) #> Reading data... #> Writing data... #> ✔ The SAS file is available in parquet format under /tmp/RtmpEZfIkY #> Writing data...  # Conversion from a SAS file to a partitioned parquet file  :  table_to_parquet(   path_to_table = system.file(\"examples\",\"iris.sas7bdat\", package = \"haven\"),   path_to_parquet = tempdir(),   partition = \"yes\",   partitioning =  c(\"Species\") # vector use as partition key ) #> Reading data... #> Writing data... #> ✔ The SAS file is available in parquet format under /tmp/RtmpEZfIkY #> Writing data...  if (FALSE) { # Reading SAS file by chunk of 50 lines # and conversion to multiple files with zstd, compression level 10  table_to_parquet(   path_to_table = system.file(\"examples\",\"iris.sas7bdat\", package = \"haven\"),   path_to_parquet = tempdir(),   chunk_size = 50,   compression = \"zstd\",   compression_level = 10 ) }"},{"path":"/news/index.html","id":"parquetize-055","dir":"Changelog","previous_headings":"","what":"parquetize 0.5.5","title":"parquetize 0.5.5","text":"CRAN release: 2023-03-28 release includes :","code":""},{"path":"/news/index.html","id":"a-very-important-new-contributor-to-parquetize--0-5-5","dir":"Changelog","previous_headings":"","what":"A very important new contributor to parquetize !","title":"parquetize 0.5.5","text":"Due numerous contributions, @nbc now officially part project authors !","code":""},{"path":"/news/index.html","id":"three-arguments-deprecation-0-5-5","dir":"Changelog","previous_headings":"","what":"Three arguments deprecation","title":"parquetize 0.5.5","text":"big refactoring, three arguments deprecated : by_chunk : table_to_parquet automatically chunked use one chunk_memory_size chunk_size. csv_as_a_zip: csv_to_table detect file zip extension. url_to_csv : use path_to_csv instead, csv_to_table detect file remote file path. raise deprecation warning moment.","code":""},{"path":"/news/index.html","id":"chunking-by-memory-size-0-5-5","dir":"Changelog","previous_headings":"","what":"Chunking by memory size","title":"parquetize 0.5.5","text":"possibility chunk parquet memory size table_to_parquet(): table_to_parquet() takes chunk_memory_size argument convert input file parquet file roughly chunk_memory_size Mb size data loaded memory. Argument by_chunk deprecated (see ). Example use argument chunk_memory_size:","code":"table_to_parquet(   path_to_table = system.file(\"examples\",\"iris.sas7bdat\", package = \"haven\"),   path_to_parquet = tempdir(),   chunk_memory_size = 5000, # this will create files of around 5Gb when loaded in memory )"},{"path":"/news/index.html","id":"passing-argument-like-compression-to-write_parquet-when-chunking-0-5-5","dir":"Changelog","previous_headings":"","what":"Passing argument like compression to write_parquet when chunking","title":"parquetize 0.5.5","text":"functionality users pass argument write_parquet() chunking argument (ellipsis). Can used example pass compression compression_level. Example:","code":"table_to_parquet(   path_to_table = system.file(\"examples\",\"iris.sas7bdat\", package = \"haven\"),   path_to_parquet = tempdir(),   compression = \"zstd\",   compression_level = 10,   chunk_memory_size = 5000 )"},{"path":"/news/index.html","id":"a-new-function-download_extract-0-5-5","dir":"Changelog","previous_headings":"","what":"A new function download_extract","title":"parquetize 0.5.5","text":"function added … download unzip file needed.","code":"file_path <- download_extract(   \"https://www.nomisweb.co.uk/output/census/2021/census2021-ts007.zip\",   filename_in_zip = \"census2021-ts007-ctry.csv\" ) csv_to_parquet(   file_path,   path_to_parquet = tempdir() )"},{"path":"/news/index.html","id":"other-0-5-5","dir":"Changelog","previous_headings":"","what":"Other","title":"parquetize 0.5.5","text":"cover, release hardened tests","code":""},{"path":"/news/index.html","id":"parquetize-054","dir":"Changelog","previous_headings":"","what":"parquetize 0.5.4","title":"parquetize 0.5.4","text":"CRAN release: 2023-03-13 release fix error converting sas file chunk.","code":""},{"path":"/news/index.html","id":"parquetize-053","dir":"Changelog","previous_headings":"","what":"parquetize 0.5.3","title":"parquetize 0.5.3","text":"CRAN release: 2023-02-20 release includes : Added columns selection table_to_parquet() csv_to_parquet() functions #20 example files parquet format iris table migrated inst/extdata directory.","code":""},{"path":"/news/index.html","id":"parquetize-052","dir":"Changelog","previous_headings":"","what":"parquetize 0.5.2","title":"parquetize 0.5.2","text":"release includes : behaviour table_to_parquet() function fixed argument by_chunk TRUE.","code":""},{"path":"/news/index.html","id":"parquetize-051","dir":"Changelog","previous_headings":"","what":"parquetize 0.5.1","title":"parquetize 0.5.1","text":"CRAN release: 2023-01-30 release removes duckdb_to_parquet() function advice Brian Ripley CRAN. Indeed, storage DuckDB yet stable. storage stabilized version 1.0 releases.","code":""},{"path":"/news/index.html","id":"parquetize-050","dir":"Changelog","previous_headings":"","what":"parquetize 0.5.0","title":"parquetize 0.5.0","text":"CRAN release: 2023-01-13 release includes corrections CRAN submission.","code":""},{"path":"/news/index.html","id":"parquetize-040","dir":"Changelog","previous_headings":"","what":"parquetize 0.4.0","title":"parquetize 0.4.0","text":"release includes important feature : table_to_parquet() function can now convert tables parquet format less memory consumption. Useful huge tables computers little RAM. (#15) vignette written . See . Removal nb_rows argument table_to_parquet() function Replaced new arguments by_chunk, chunk_size skip (see documentation) Progress bars now managed {cli} package","code":""},{"path":"/news/index.html","id":"parquetize-030","dir":"Changelog","previous_headings":"","what":"parquetize 0.3.0","title":"parquetize 0.3.0","text":"Added duckdb_to_parquet() function convert duckdb files parquet format. Added sqlite_to_parquet() function convert sqlite files parquet format.","code":""},{"path":"/news/index.html","id":"parquetize-020","dir":"Changelog","previous_headings":"","what":"parquetize 0.2.0","title":"parquetize 0.2.0","text":"Added rds_to_parquet() function convert rds files parquet format. Added json_to_parquet() function convert json ndjson files parquet format. Added possibility convert csv file partitioned parquet file. Improving code coverage (#9) Check path_to_parquet exists functions csv_to_parquet() table_to_parquet() (@py-b)","code":""},{"path":"/news/index.html","id":"parquetize-010","dir":"Changelog","previous_headings":"","what":"parquetize 0.1.0","title":"parquetize 0.1.0","text":"Added table_to_parquet() function convert SAS, SPSS Stata files parquet format. Added csv_to_parquet() function convert csv files parquet format. Added parquetize_example() function get path package data examples. Added NEWS.md file track changes package.","code":""}]
