[{"path":"/CONTRIBUTING.html","id":null,"dir":"","previous_headings":"","what":"Contributing to parquetize development","title":"Contributing to parquetize development","text":"goal guide help get contributing parquetize quickly possible. guide divided two main pieces: Filing bug report feature request issue. Suggesting change via pull request.","code":""},{"path":"/CONTRIBUTING.html","id":"issues","dir":"","previous_headings":"","what":"Issues","title":"Contributing to parquetize development","text":"filing issue, important thing include minimal reproducible example can quickly verify problem, figure fix . three things need include make example reproducible: required packages, data, code. Packages loaded top script, ’s easy see ones example needs. easiest way include data use dput() generate R code recreate . example, recreate mtcars dataset R, ’d perform following steps: Run dput(mtcars) R Copy output reproducible script, type mtcars <- paste. even better can create data.frame() just handful rows columns still illustrates problem. Spend little bit time ensuring code easy others read: make sure ’ve used spaces variable names concise, informative use comments indicate problem lies best remove everything related problem. shorter code , easier understand. can check actually made reproducible example starting fresh R session pasting script . (Unless ’ve specifically asked , please don’t include output sessionInfo().)","code":""},{"path":"/CONTRIBUTING.html","id":"pull-requests","dir":"","previous_headings":"","what":"Pull requests","title":"Contributing to parquetize development","text":"contribute change parquetize, follow steps: Create branch git make changes. Push branch github issue pull request (PR). Discuss pull request. Iterate either accept PR decide ’s good fit parquetize. steps described detail . might feel overwhelming first time get set , gets easier practice. ’re familiar git github, please start reading http://r-pkgs..co.nz/git.html Pull requests evaluated seven point checklist: Motivation. pull request clearly concisely motivate need change. Also include motivation NEWS new release parquetize comes ’s easy users see ’s changed. Add item top file use markdown formatting. news item end (@yourGithubUsername, #the_issue_number). related changes. submit pull request, please check make sure haven’t accidentally included unrelated changes. make harder see exactly ’s changed, evaluate unexpected side effects. PR corresponds git branch, expect submit multiple changes make sure create multiple branches. multiple changes depend , start first one don’t submit others first one processed. ’re adding new parameters new function, ’ll also need document roxygen. Make sure re-run devtools::document() code submitting. fixing bug adding new feature, please add testthat unit test. seems like lot work don’t worry pull request isn’t perfect. pull request (“PR”) process, unless ’ve submitted past ’s unlikely pull request accepted . Many thanks advance !","code":""},{"path":[]},{"path":"/articles/aa-conversions.html","id":"with-table_to_parquet","dir":"Articles","previous_headings":"Convert huge input file to parquet format","what":"With table_to_parquet()","title":"Convert huge input file to parquet","text":"huge input files SAS, SPSS Stata formats, parquetize package allows perform clever conversion using chunk_memory_size chunk_size table_to_parquet() function. native behavior function (functions package) load entire table converted R write disk (single file partitioned directory). handling large files, risk frequently occurs R session aborts load entire database memory. risk even present work locally computer can limited work remote servers.table_to_parquet() offers solution answers need expressed parquetize users. examples documentation using iris table. ’s two ways split output files : memory consumption number lines","code":""},{"path":"/articles/aa-conversions.html","id":"spliting-data-by-memory-consumption","dir":"Articles","previous_headings":"Convert huge input file to parquet format > With table_to_parquet()","what":"Spliting data by memory consumption","title":"Convert huge input file to parquet","text":"table_to_parquet can guess number lines put file based memory consuption argument chunk_memory_size expressed Mb. cut 150 rows chunks roughly 5 Kb file loaded tibble. example get 2 parquet files 89 lines called iris1-89.parquet iris90-150.parquet real life, use chunk_memory_size Gb range, example SAS file 50 000 000 lines using chunk_memory_size 5000 Mb :","code":"table_to_parquet(   path_to_file = system.file(\"examples\", \"iris.sas7bdat\", package = \"haven\"),   path_to_parquet = tempfile(),   max_memory = 5 / 1024,   encoding = \"utf-8\" ) #> Reading data... #> Writing file16a77f901466-1-89.parquet... #> Reading data... #> Writing file16a77f901466-90-150.parquet... #> ✔ Data are available in parquet dataset under /tmp/Rtmpu1vXFY/file16a77f901466/ #> Writing file16a77f901466-90-150.parquet... table_to_parquet(   path_to_file = \"myhugefile.sas7bdat\",   path_to_parquet = tempdir(),   max_memory = 5000,   encoding = \"utf-8\" )"},{"path":"/articles/aa-conversions.html","id":"splitting-data-by-number-of-lines","dir":"Articles","previous_headings":"Convert huge input file to parquet format > With table_to_parquet()","what":"Splitting data by number of lines","title":"Convert huge input file to parquet","text":"Tip: number lines chunk must contain must supported RAM computer/server. Ideally, number chunks defined must limited. tens hundreds limit number intermediate files (see example ). cut 150 rows 3 chunks 50 rows. example get 3 parquet files 50 lines called iris1-50.parquet, iris51-100.parquet iris101-151.parquet real life, can perform kind request parquetize API (example SAS file 50 000 000 lines defining 25 chunks 2 000 000 rows ) : Files myhugefile1-2000000.parquet, myhugefile2000001-4000000.parquet … created.","code":"table_to_parquet(   path_to_file = system.file(\"examples\", \"iris.sas7bdat\", package = \"haven\"),   path_to_parquet = tempfile(),   max_rows = 50,   encoding = \"utf-8\" ) table_to_parquet(   path_to_file = \"myhugefile.sas7bdat\",   path_to_parquet = tempdir(),   max_rows = 2000000,   encoding = \"utf-8\" )"},{"path":"/articles/aa-conversions.html","id":"function-rbind_parquet","dir":"Articles","previous_headings":"Convert huge input file to parquet format","what":"Function rbind_parquet()","title":"Convert huge input file to parquet","text":"end conversion table_to_parquet(), want reconstitute unique initial table computer resources (RAM) , can use helper function provided API rbind_parquet(). function allows bind multiple parquet files row. ’s example without deleting initial files (delete_initial_files=FALSE) : myhugefile.parquet file created myhugefile1-2000000.parquet, myhugefile2000001-4000000.parquet… files!","code":"rbind_parquet(   folder = tempfile(),   output_name = \"myhugefile\",   delete_initial_files = FALSE )"},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Damien Dotta. Author, maintainer. Nicolas Chuche. Author.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Dotta D, Chuche N (2023). parquetize: Convert Files Parquet Format. https://ddotta.github.io/parquetize/, https://github.com/ddotta/parquetize.","code":"@Manual{,   title = {parquetize: Convert Files to Parquet Format},   author = {Damien Dotta and Nicolas Chuche},   year = {2023},   note = {https://ddotta.github.io/parquetize/, https://github.com/ddotta/parquetize}, }"},{"path":"/index.html","id":"package-package-parquetize-","dir":"","previous_headings":"","what":"Convert Files to Parquet Format","title":"Convert Files to Parquet Format","text":"R package allows convert databases different formats (csv, SAS, SPSS, Stata, rds, sqlite, JSON, ndJSON) parquet format function.","code":""},{"path":"/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Convert Files to Parquet Format","text":"install parquetize CRAN : alternatively install development version GitHub : load :","code":"install.packages(\"parquetize\") remotes::install_github(\"ddotta/parquetize\") library(parquetize)"},{"path":"/index.html","id":"why-this-package-","dir":"","previous_headings":"","what":"Why this package ?","title":"Convert Files to Parquet Format","text":"package simple wrapper useful functions haven, readr, jsonlite, RSQLite arrow packages. working, realized often repeating operation working parquet files : import file R {haven}, {jsonlite}, {readr}, {DBI} {RSQLite}. export file parquet format fervent DRY principle (don’t repeat ) exported functions package make life easier execute operations within function. last benefit using package parquetize functions allow create single parquet files partitioned files depending arguments chosen functions. benefit function allows convert csv files whether stored locally available internet directly csv format inside zip. benefit function handles JSON ndJSON files function. one function use 2 cases. rds_to_parquet() fst_to_parquet() benefit function handles SAS, SPSS Stata files function. one function use 3 cases. avoid overcharging R’s RAM huge table, conversion can done chunk. information, see sqlite_to_parquet() dbi_to_parquet() details, see examples associated function documentation.","code":""},{"path":"/index.html","id":"example","dir":"","previous_headings":"","what":"Example","title":"Convert Files to Parquet Format","text":"want use Insee file first names birth department? Use R {parquetize} package takes care everything: downloads data (3.7 million rows) converts parquet format seconds !","code":""},{"path":"/index.html","id":"contribution","dir":"","previous_headings":"","what":"Contribution","title":"Convert Files to Parquet Format","text":"Feel welcome contribute add features find useful daily work. Ideas welcomed issues.","code":""},{"path":"/reference/check_parquet.html","id":null,"dir":"Reference","previous_headings":"","what":"Check if parquet file or dataset is readable and return basic informations — check_parquet","title":"Check if parquet file or dataset is readable and return basic informations — check_parquet","text":"function checks file/dataset valid parquet format.   print number lines/columns return tibble columns   information.","code":""},{"path":"/reference/check_parquet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check if parquet file or dataset is readable and return basic informations — check_parquet","text":"","code":"check_parquet(path)"},{"path":"/reference/check_parquet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check if parquet file or dataset is readable and return basic informations — check_parquet","text":"path path file dataset","code":""},{"path":"/reference/check_parquet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check if parquet file or dataset is readable and return basic informations — check_parquet","text":"tibble information parquet dataset/file's columns   three columns : field name, arrow type nullable","code":""},{"path":"/reference/check_parquet.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Check if parquet file or dataset is readable and return basic informations — check_parquet","text":"function : * open parquet dataset/file check valid * print number lines * print number columns * return tibble 2 columns : * column name (string)   * arrow type (string) can find list arrow type documentation page.","code":""},{"path":"/reference/check_parquet.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Check if parquet file or dataset is readable and return basic informations — check_parquet","text":"","code":"# check a parquet file check_parquet(parquetize_example(\"iris.parquet\")) #> ℹ checking: /home/runner/work/_temp/Library/parquetize/extdata/iris.parquet #> ✔ loading dataset:   ok #> ✔ number of lines:   150 #> ✔ number of columns: 5 #> # A tibble: 5 × 2 #>   name         type       #>   <chr>        <chr>      #> 1 Sepal.Length double     #> 2 Sepal.Width  double     #> 3 Petal.Length double     #> 4 Petal.Width  double     #> 5 Species      dictionary  # check a parquet dataset check_parquet(parquetize_example(\"iris_dataset\")) #> ℹ checking: /home/runner/work/_temp/Library/parquetize/extdata/iris_dataset #> ✔ loading dataset:   ok #> ✔ number of lines:   150 #> ✔ number of columns: 5 #> # A tibble: 5 × 2 #>   name         type   #>   <chr>        <chr>  #> 1 Sepal.Length double #> 2 Sepal.Width  double #> 3 Petal.Length double #> 4 Petal.Width  double #> 5 Species      utf8"},{"path":"/reference/csv_to_parquet.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert a csv file to parquet format — csv_to_parquet","title":"Convert a csv file to parquet format — csv_to_parquet","text":"function allows convert csv file parquet format. Two conversions possibilities offered : Convert single parquet file. Argument `path_to_parquet` must used; Convert partitioned parquet file. Additionnal arguments `partition` `partitioning` must used;","code":""},{"path":"/reference/csv_to_parquet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert a csv file to parquet format — csv_to_parquet","text":"","code":"csv_to_parquet(   path_to_file,   url_to_csv = lifecycle::deprecated(),   csv_as_a_zip = lifecycle::deprecated(),   filename_in_zip,   path_to_parquet,   columns = \"all\",   compression = \"snappy\",   compression_level = NULL,   partition = \"no\",   encoding = \"UTF-8\",   ... )"},{"path":"/reference/csv_to_parquet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert a csv file to parquet format — csv_to_parquet","text":"path_to_file String indicates path input file (forget extension). url_to_csv DEPRECATED use path_to_file instead csv_as_a_zip DEPRECATED filename_in_zip name csv file zip. Required several csv included zip. path_to_parquet String indicates path directory parquet files stored. columns Character vector columns select input file (default, columns selected). compression compression algorithm. Default \"snappy\". compression_level compression level. Meaning depends compression algorithm. partition String (\"yes\" \"\" - default) indicates whether want create partitioned parquet file. \"yes\", `\"partitioning\"` argument must filled . case, folder created modality variable filled `\"partitioning\"`. careful, argument can \"yes\" `max_memory` `max_rows` argument NULL. encoding String indicates character encoding input file. ... additional format-specific arguments, see arrow::write_parquet() arrow::write_dataset() informations.","code":""},{"path":"/reference/csv_to_parquet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert a csv file to parquet format — csv_to_parquet","text":"parquet file, invisibly","code":""},{"path":"/reference/csv_to_parquet.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Convert a csv file to parquet format — csv_to_parquet","text":"careful, zip size exceeds 4 GB, function may truncate data (unzip() work reliably case - see ). case, advised unzip csv file hand (example 7-Zip) use function argument `path_to_file`.","code":""},{"path":"/reference/csv_to_parquet.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert a csv file to parquet format — csv_to_parquet","text":"","code":"# Conversion from a local csv file to a single parquet file :  csv_to_parquet(   path_to_file = parquetize_example(\"region_2022.csv\"),   path_to_parquet = tempfile(fileext=\".parquet\") ) #> Reading data... #> Writing data... #> ✔ Data are available in parquet file under /tmp/RtmpBXcUtf/file151013836d9d.parquet #> Writing data... #> Reading data...  # Conversion from a local csv file to a single parquet file and select only # few columns :  csv_to_parquet(   path_to_file = parquetize_example(\"region_2022.csv\"),   path_to_parquet = tempfile(fileext = \".parquet\"),   columns = c(\"REG\",\"LIBELLE\") ) #> Reading data... #> Writing data... #> ✔ Data are available in parquet file under /tmp/RtmpBXcUtf/file15105a9ab6d9.parquet #> Writing data... #> Reading data...  # Conversion from a local csv file to a partitioned parquet file  :  csv_to_parquet(   path_to_file = parquetize_example(\"region_2022.csv\"),   path_to_parquet = tempfile(fileext = \".parquet\"),   partition = \"yes\",   partitioning =  c(\"REG\") ) #> Reading data... #> Writing data... #> ✔ Data are available in parquet dataset under /tmp/RtmpBXcUtf/file15106cf23006.parquet #> Writing data... #> Reading data...  # Conversion from a URL and a csv file with \"gzip\" compression :  csv_to_parquet(   path_to_file =   \"https://github.com/sidsriv/Introduction-to-Data-Science-in-python/raw/master/census.csv\",   path_to_parquet = tempfile(fileext = \".parquet\"),   compression = \"gzip\",   compression_level = 5 ) #> Reading data... #> Writing data... #> ✔ Data are available in parquet file under /tmp/RtmpBXcUtf/file15105576c364.parquet #> Writing data... #> Reading data...  # Conversion from a URL and a zipped file :  csv_to_parquet(   path_to_file = \"https://www.nomisweb.co.uk/output/census/2021/census2021-ts007.zip\",   filename_in_zip = \"census2021-ts007-ctry.csv\",   path_to_parquet = tempfile(fileext = \".parquet\") ) #> Reading data... #> Writing data... #> ✔ Data are available in parquet file under /tmp/RtmpBXcUtf/file1510865438b.parquet #> Writing data... #> Reading data..."},{"path":"/reference/dbi_to_parquet.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert a SQL Query on a DBI connection to parquet format — dbi_to_parquet","title":"Convert a SQL Query on a DBI connection to parquet format — dbi_to_parquet","text":"function allows convert SQL query DBI parquet format. handles DBI supported databases. Two conversions possibilities offered : Convert single parquet file. Argument `path_to_parquet` must used; Convert partitioned parquet file. Additionnal arguments `partition` `partitioning` must used; Examples explain convert query chunked dataset","code":""},{"path":"/reference/dbi_to_parquet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert a SQL Query on a DBI connection to parquet format — dbi_to_parquet","text":"","code":"dbi_to_parquet(   conn,   sql_query,   path_to_parquet,   max_memory,   max_rows,   chunk_memory_sample_lines = 10000,   partition = \"no\",   compression = \"snappy\",   compression_level = NULL,   ... )"},{"path":"/reference/dbi_to_parquet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert a SQL Query on a DBI connection to parquet format — dbi_to_parquet","text":"conn DBIConnection object, return DBI::dbConnect sql_query character string containing SQL query (argument passed DBI::dbSendQuery) path_to_parquet String indicates path directory parquet files stored. max_memory Memory size (Mb) data one parquet file roughly fit. max_rows Number lines defines size chunk. argument can filled max_memory used. chunk_memory_sample_lines Number lines read evaluate max_memory. Default 10 000. partition String (\"yes\" \"\" - default) indicates whether want create partitioned parquet file. \"yes\", `\"partitioning\"` argument must filled . case, folder created modality variable filled `\"partitioning\"`. careful, argument can \"yes\" `max_memory` `max_rows` argument NULL. compression compression algorithm. Default \"snappy\". compression_level compression level. Meaning depends compression algorithm. ... additional format-specific arguments, see arrow::write_parquet() arrow::write_dataset() informations.","code":""},{"path":"/reference/dbi_to_parquet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert a SQL Query on a DBI connection to parquet format — dbi_to_parquet","text":"parquet file, invisibly","code":""},{"path":"/reference/dbi_to_parquet.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert a SQL Query on a DBI connection to parquet format — dbi_to_parquet","text":"","code":"# Conversion from a sqlite dbi connection to a single parquet file :  dbi_connection <- DBI::dbConnect(RSQLite::SQLite(),   system.file(\"extdata\",\"iris.sqlite\",package = \"parquetize\"))  # Reading iris table from local sqlite database # and conversion to one parquet file :  dbi_to_parquet(   conn = dbi_connection,   sql_query = \"SELECT * FROM iris\",   path_to_parquet = tempfile(fileext=\".parquet\"), ) #> Reading data... #> Writing data... #> ✔ Data are available in parquet file under /tmp/RtmpBXcUtf/file1510252e74bf.parquet #> Writing data... #> Reading data...  # Reading iris table from local sqlite database by chunk (using # `max_memory` argument) and conversion to multiple parquet files  dbi_to_parquet(   conn = dbi_connection,   sql_query = \"SELECT * FROM iris\",   path_to_parquet = tempdir(),   max_memory = 2 / 1024 ) #> Reading data... #> Writing data in part-1-42.parquet... #> Reading data... #> Writing data in part-43-84.parquet... #> Reading data... #> Writing data in part-85-126.parquet... #> Reading data... #> Writing data in part-127-150.parquet... #> ✔ Parquet dataset is available under /tmp/RtmpBXcUtf/ #> Writing data in part-127-150.parquet...  # Using chunk and partition together is not possible directly but easy to do : # Reading iris table from local sqlite database by chunk (using # `max_memory` argument) and conversion to arrow dataset partitioned by # species  # get unique values of column \"iris from table \"iris\" partitions <- get_partitions(dbi_connection, table = \"iris\", column = \"Species\")  # loop over those values for (species in partitions) {   dbi_to_parquet(     conn = dbi_connection,     # use glue_sql to create the query filtering the partition     sql_query = glue::glue_sql(\"SELECT * FROM iris where Species = {species}\",                                .con = dbi_connection),     # add the partition name in the output dir to respect parquet partition schema     path_to_parquet = file.path(tempdir(), \"iris\", paste0(\"Species=\", species)),     max_memory = 2 / 1024,   ) } #> Reading data... #> Writing data in part-1-31.parquet... #> Reading data... #> Writing data in part-32-50.parquet... #> ✔ Parquet dataset is available under /tmp/RtmpBXcUtf/iris/Species=setosa/ #> Writing data in part-32-50.parquet... #> Reading data... #> Writing data in part-1-31.parquet... #> Reading data... #> Writing data in part-32-50.parquet... #> ✔ Parquet dataset is available under /tmp/RtmpBXcUtf/iris/Species=versicolor/ #> Writing data in part-32-50.parquet... #> Reading data... #> Writing data in part-1-31.parquet... #> Reading data... #> Writing data in part-32-50.parquet... #> ✔ Parquet dataset is available under /tmp/RtmpBXcUtf/iris/Species=virginica/ #> Writing data in part-32-50.parquet...  # If you need a more complicated query to get your partitions, you can use # dbGetQuery directly : col_to_partition <- DBI::dbGetQuery(dbi_connection, \"SELECT distinct(`Species`) FROM `iris`\")[,1]"},{"path":"/reference/download_extract.html","id":null,"dir":"Reference","previous_headings":"","what":"download and uncompress file if needed — download_extract","title":"download and uncompress file if needed — download_extract","text":"function download file file remote   unzip zipped.  just return input path argument   neither. zip contains multiple files, can use `filename_in_zip` set file want unzip use. can pipe output `*_to_parquet` functions.","code":""},{"path":"/reference/download_extract.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"download and uncompress file if needed — download_extract","text":"","code":"download_extract(path, filename_in_zip)"},{"path":"/reference/download_extract.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"download and uncompress file if needed — download_extract","text":"path input  file's path url. filename_in_zip name csv file zip. Required several csv included zip.","code":""},{"path":"/reference/download_extract.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"download and uncompress file if needed — download_extract","text":"path usable (uncompressed) file, invisibly.","code":""},{"path":"/reference/download_extract.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"download and uncompress file if needed — download_extract","text":"","code":"# 1. unzip a local zip file # 2. parquetize it  file_path <- download_extract(system.file(\"extdata\",\"mtcars.csv.zip\", package = \"readr\")) csv_to_parquet(   file_path,   path_to_parquet = tempfile(fileext = \".parquet\") ) #> Reading data... #> Writing data... #> ✔ Data are available in parquet file under /tmp/RtmpBXcUtf/file1510130b7c46.parquet #> Writing data... #> Reading data...  # 1. download a remote file # 2. extract the file census2021-ts007-ctry.csv # 3. parquetize it  file_path <- download_extract(   \"https://www.nomisweb.co.uk/output/census/2021/census2021-ts007.zip\",   filename_in_zip = \"census2021-ts007-ctry.csv\" ) csv_to_parquet(   file_path,   path_to_parquet = tempfile(fileext = \".parquet\") ) #> Reading data... #> Writing data... #> ✔ Data are available in parquet file under /tmp/RtmpBXcUtf/file15102039ea2a.parquet #> Writing data... #> Reading data...  # the file is local and not zipped so : # 1. parquetize it  file_path <- download_extract(parquetize_example(\"region_2022.csv\")) csv_to_parquet(   file_path,   path_to_parquet = tempfile(fileext = \".parquet\") ) #> Reading data... #> Writing data... #> ✔ Data are available in parquet file under /tmp/RtmpBXcUtf/file151076458c31.parquet #> Writing data... #> Reading data..."},{"path":"/reference/expect_missing_argument.html","id":null,"dir":"Reference","previous_headings":"","what":"Check if missing argument error is raised — expect_missing_argument","title":"Check if missing argument error is raised — expect_missing_argument","text":"Check missing argument error raised","code":""},{"path":"/reference/expect_missing_argument.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check if missing argument error is raised — expect_missing_argument","text":"","code":"expect_missing_argument(object, regexp)"},{"path":"/reference/expect_missing_argument.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check if missing argument error is raised — expect_missing_argument","text":"object object check message regexp message must find","code":""},{"path":"/reference/expect_missing_argument.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check if missing argument error is raised — expect_missing_argument","text":"expect_error","code":""},{"path":"/reference/expect_parquet.html","id":null,"dir":"Reference","previous_headings":"","what":"Check if parquet dataset/file is readable and has the good number of rows — expect_parquet","title":"Check if parquet dataset/file is readable and has the good number of rows — expect_parquet","text":"Check parquet dataset/file readable good number rows","code":""},{"path":"/reference/expect_parquet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check if parquet dataset/file is readable and has the good number of rows — expect_parquet","text":"","code":"expect_parquet(   path,   with_lines,   with_partitions = NULL,   with_columns = NULL,   with_files = NULL )"},{"path":"/reference/expect_parquet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check if parquet dataset/file is readable and has the good number of rows — expect_parquet","text":"path parquet file dataset with_lines number lines file/dataset with_partitions NULL vector partition names dataset with_columns NULL column's name vector dataset/file with_files NULL number files dataset ","code":""},{"path":"/reference/expect_parquet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check if parquet dataset/file is readable and has the good number of rows — expect_parquet","text":"dataset handle","code":""},{"path":"/reference/fst_to_parquet.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert a fst file to parquet format — fst_to_parquet","title":"Convert a fst file to parquet format — fst_to_parquet","text":"function allows convert fst file parquet format. Two conversions possibilities offered : Convert single parquet file. Argument `path_to_parquet` must used; Convert partitioned parquet file. Additionnal arguments `partition` `partitioning` must used;","code":""},{"path":"/reference/fst_to_parquet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert a fst file to parquet format — fst_to_parquet","text":"","code":"fst_to_parquet(   path_to_file,   path_to_parquet,   partition = \"no\",   compression = \"snappy\",   compression_level = NULL,   ... )"},{"path":"/reference/fst_to_parquet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert a fst file to parquet format — fst_to_parquet","text":"path_to_file String indicates path input file (forget extension). path_to_parquet String indicates path directory parquet files stored. partition String (\"yes\" \"\" - default) indicates whether want create partitioned parquet file. \"yes\", `\"partitioning\"` argument must filled . case, folder created modality variable filled `\"partitioning\"`. careful, argument can \"yes\" `max_memory` `max_rows` argument NULL. compression compression algorithm. Default \"snappy\". compression_level compression level. Meaning depends compression algorithm. ... additional format-specific arguments, see arrow::write_parquet() arrow::write_dataset() informations.","code":""},{"path":"/reference/fst_to_parquet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert a fst file to parquet format — fst_to_parquet","text":"parquet file, invisibly","code":""},{"path":"/reference/fst_to_parquet.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert a fst file to parquet format — fst_to_parquet","text":"","code":"# Conversion from a local fst file to a single parquet file ::  fst_to_parquet(   path_to_file = system.file(\"extdata\",\"iris.fst\",package = \"parquetize\"),   path_to_parquet = tempfile(fileext = \".parquet\") ) #> Reading data... #> Writing data... #> ✔ Data are available in parquet file under /tmp/RtmpBXcUtf/file1510439b19cc.parquet #> Writing data... #> Reading data...  # Conversion from a local fst file to a partitioned parquet file  ::  fst_to_parquet(   path_to_file = system.file(\"extdata\",\"iris.fst\",package = \"parquetize\"),   path_to_parquet = tempfile(fileext = \".parquet\"),   partition = \"yes\",   partitioning =  c(\"Species\") ) #> Reading data... #> Writing data... #> ✔ Data are available in parquet dataset under /tmp/RtmpBXcUtf/file151052b1765.parquet #> Writing data... #> Reading data..."},{"path":"/reference/get_partitions.html","id":null,"dir":"Reference","previous_headings":"","what":"get unique values from table's column — get_partitions","title":"get unique values from table's column — get_partitions","text":"function allows extract unique values table's column use partitions. Internally, function \"SELECT DISTINCT(`mycolumn`) `mytable`;\"","code":""},{"path":"/reference/get_partitions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"get unique values from table's column — get_partitions","text":"","code":"get_partitions(conn, table, column)"},{"path":"/reference/get_partitions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"get unique values from table's column — get_partitions","text":"conn `DBIConnection` object, return `DBI::dbConnect` table DB table name column column name table passed param","code":""},{"path":"/reference/get_partitions.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"get unique values from table's column — get_partitions","text":"vector unique values column table","code":""},{"path":"/reference/get_partitions.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"get unique values from table's column — get_partitions","text":"","code":"dbi_connection <- DBI::dbConnect(RSQLite::SQLite(),   system.file(\"extdata\",\"iris.sqlite\",package = \"parquetize\"))  get_partitions(dbi_connection, \"iris\", \"Species\") #> [1] \"setosa\"     \"versicolor\" \"virginica\""},{"path":"/reference/json_to_parquet.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert a json file to parquet format — json_to_parquet","title":"Convert a json file to parquet format — json_to_parquet","text":"function allows convert json ndjson file parquet format. Two conversions possibilities offered : Convert single parquet file. Argument `path_to_parquet` must used; Convert partitioned parquet file. Additionnal arguments `partition` `partitioning` must used;","code":""},{"path":"/reference/json_to_parquet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert a json file to parquet format — json_to_parquet","text":"","code":"json_to_parquet(   path_to_file,   path_to_parquet,   format = \"json\",   partition = \"no\",   compression = \"snappy\",   compression_level = NULL,   ... )"},{"path":"/reference/json_to_parquet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert a json file to parquet format — json_to_parquet","text":"path_to_file String indicates path input file (forget extension). path_to_parquet String indicates path directory parquet files stored. format string indicates format \"json\" (default) \"ndjson\" partition String (\"yes\" \"\" - default) indicates whether want create partitioned parquet file. \"yes\", `\"partitioning\"` argument must filled . case, folder created modality variable filled `\"partitioning\"`. careful, argument can \"yes\" `max_memory` `max_rows` argument NULL. compression compression algorithm. Default \"snappy\". compression_level compression level. Meaning depends compression algorithm. ... additional format-specific arguments, see arrow::write_parquet() arrow::write_dataset() informations.","code":""},{"path":"/reference/json_to_parquet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert a json file to parquet format — json_to_parquet","text":"parquet file, invisibly","code":""},{"path":"/reference/json_to_parquet.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert a json file to parquet format — json_to_parquet","text":"","code":"# Conversion from a local json file to a single parquet file ::  json_to_parquet(   path_to_file = system.file(\"extdata\",\"iris.json\",package = \"parquetize\"),   path_to_parquet = tempfile(fileext = \".parquet\") ) #> Reading data... #> Writing data... #> ✔ Data are available in parquet file under /tmp/RtmpBXcUtf/file151031177f38.parquet #> Writing data... #> Reading data...  # Conversion from a local ndjson file to a partitioned parquet file  ::  json_to_parquet(   path_to_file = system.file(\"extdata\",\"iris.ndjson\",package = \"parquetize\"),   path_to_parquet = tempfile(fileext = \".parquet\"),   format = \"ndjson\" ) #> Reading data... #> Writing data... #> ✔ Data are available in parquet file under /tmp/RtmpBXcUtf/file151017157341.parquet #> Writing data... #> Reading data..."},{"path":"/reference/parquetize-package.html","id":null,"dir":"Reference","previous_headings":"","what":"parquetize: Convert Files to Parquet Format — parquetize-package","title":"parquetize: Convert Files to Parquet Format — parquetize-package","text":"Collection functions get files parquet format. Parquet columnar storage file format https://parquet.apache.org/. files convert can several formats (\"csv\", \"RData\", \"rds\", \"RSQLite\", \"json\", \"ndjson\", \"SAS\", \"SPSS\"...).","code":""},{"path":[]},{"path":"/reference/parquetize-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"parquetize: Convert Files to Parquet Format — parquetize-package","text":"Maintainer: Damien Dotta damien.dotta@live.fr Authors: Nicolas Chuche nicolas.chuche@barna.","code":""},{"path":"/reference/parquetize_example.html","id":null,"dir":"Reference","previous_headings":"","what":"Get path to parquetize example — parquetize_example","title":"Get path to parquetize example — parquetize_example","text":"parquetize comes bundled number sample files `inst/extdata` directory. function make easy access","code":""},{"path":"/reference/parquetize_example.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get path to parquetize example — parquetize_example","text":"","code":"parquetize_example(file = NULL)"},{"path":"/reference/parquetize_example.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get path to parquetize example — parquetize_example","text":"file Name file directory. `NULL`, example files listed.","code":""},{"path":"/reference/parquetize_example.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get path to parquetize example — parquetize_example","text":"character string","code":""},{"path":"/reference/parquetize_example.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get path to parquetize example — parquetize_example","text":"","code":"parquetize_example() #> [1] \"iris.duckdb\"     \"iris.fst\"        \"iris.json\"       \"iris.ndjson\"     #> [5] \"iris.parquet\"    \"iris.rds\"        \"iris.sqlite\"     \"iris_dataset\"    #> [9] \"region_2022.csv\" parquetize_example(\"region_2022.csv\") #> [1] \"/home/runner/work/_temp/Library/parquetize/extdata/region_2022.csv\" parquetize_example(\"iris_dataset\") #> [1] \"/home/runner/work/_temp/Library/parquetize/extdata/iris_dataset\""},{"path":"/reference/rbind_parquet.html","id":null,"dir":"Reference","previous_headings":"","what":"Function to bind multiple parquet files by row — rbind_parquet","title":"Function to bind multiple parquet files by row — rbind_parquet","text":"function read parquet files `folder` argument starts `output_name`, combine using rbind write result new parquet file. can also delete initial files `delete_initial_files` argument TRUE. careful, function work files different structures present folder given argument `folder`.","code":""},{"path":"/reference/rbind_parquet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Function to bind multiple parquet files by row — rbind_parquet","text":"","code":"rbind_parquet(   folder,   output_name,   delete_initial_files = TRUE,   compression = \"snappy\",   compression_level = NULL )"},{"path":"/reference/rbind_parquet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Function to bind multiple parquet files by row — rbind_parquet","text":"folder folder initial files stored output_name name output parquet file delete_initial_files Boolean. function delete initial files ? default TRUE. compression compression algorithm. Default \"snappy\". compression_level compression level. Meaning depends compression algorithm.","code":""},{"path":"/reference/rbind_parquet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Function to bind multiple parquet files by row — rbind_parquet","text":"Parquet files, invisibly","code":""},{"path":"/reference/rbind_parquet.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Function to bind multiple parquet files by row — rbind_parquet","text":"","code":"if (FALSE) { library(arrow) if (file.exists('output')==FALSE) {   dir.create(\"output\") }  file.create(fileext = \"output/test_data1-4.parquet\") write_parquet(data.frame(   x = c(\"a\",\"b\",\"c\"),   y = c(1L,2L,3L) ), \"output/test_data1-4.parquet\")  file.create(fileext = \"output/test_data4-6.parquet\") write_parquet(data.frame(   x = c(\"d\",\"e\",\"f\"),   y = c(4L,5L,6L) ), \"output/test_data4-6.parquet\")  test_data <- rbind_parquet(folder = \"output\",                            output_name = \"test_data\",                            delete_initial_files = FALSE) }"},{"path":"/reference/rds_to_parquet.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert a rds file to parquet format — rds_to_parquet","title":"Convert a rds file to parquet format — rds_to_parquet","text":"function allows convert rds file parquet format. Two conversions possibilities offered : Convert single parquet file. Argument `path_to_parquet` must used; Convert partitioned parquet file. Additionnal arguments `partition` `partitioning` must used;","code":""},{"path":"/reference/rds_to_parquet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert a rds file to parquet format — rds_to_parquet","text":"","code":"rds_to_parquet(   path_to_file,   path_to_parquet,   partition = \"no\",   compression = \"snappy\",   compression_level = NULL,   ... )"},{"path":"/reference/rds_to_parquet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert a rds file to parquet format — rds_to_parquet","text":"path_to_file String indicates path input file (forget extension). path_to_parquet String indicates path directory parquet files stored. partition String (\"yes\" \"\" - default) indicates whether want create partitioned parquet file. \"yes\", `\"partitioning\"` argument must filled . case, folder created modality variable filled `\"partitioning\"`. careful, argument can \"yes\" `max_memory` `max_rows` argument NULL. compression compression algorithm. Default \"snappy\". compression_level compression level. Meaning depends compression algorithm. ... additional format-specific arguments, see arrow::write_parquet() arrow::write_dataset() informations.","code":""},{"path":"/reference/rds_to_parquet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert a rds file to parquet format — rds_to_parquet","text":"parquet file, invisibly","code":""},{"path":"/reference/rds_to_parquet.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert a rds file to parquet format — rds_to_parquet","text":"","code":"# Conversion from a local rds file to a single parquet file ::  rds_to_parquet(   path_to_file = system.file(\"extdata\",\"iris.rds\",package = \"parquetize\"),   path_to_parquet = tempfile(fileext = \".parquet\") ) #> Reading data... #> Writing data... #> ✔ Data are available in parquet file under /tmp/RtmpBXcUtf/file151041480a39.parquet #> Writing data... #> Reading data...  # Conversion from a local rds file to a partitioned parquet file  ::  rds_to_parquet(   path_to_file = system.file(\"extdata\",\"iris.rds\",package = \"parquetize\"),   path_to_parquet = tempfile(fileext = \".parquet\"),   partition = \"yes\",   partitioning =  c(\"Species\") ) #> Reading data... #> Writing data... #> ✔ Data are available in parquet dataset under /tmp/RtmpBXcUtf/file15101f67d671.parquet #> Writing data... #> Reading data..."},{"path":"/reference/sqlite_to_parquet.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert a sqlite file to parquet format — sqlite_to_parquet","title":"Convert a sqlite file to parquet format — sqlite_to_parquet","text":"function allows convert table sqlite file parquet format.  following extensions supported : \"db\",\"sdb\",\"sqlite\",\"db3\",\"s3db\",\"sqlite3\",\"sl3\",\"db2\",\"s2db\",\"sqlite2\",\"sl2\". Two conversions possibilities offered : Convert single parquet file. Argument `path_to_parquet` must used; Convert partitioned parquet file. Additionnal arguments `partition` `partitioning` must used;","code":""},{"path":"/reference/sqlite_to_parquet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert a sqlite file to parquet format — sqlite_to_parquet","text":"","code":"sqlite_to_parquet(   path_to_file,   table_in_sqlite,   path_to_parquet,   partition = \"no\",   compression = \"snappy\",   compression_level = NULL,   ... )"},{"path":"/reference/sqlite_to_parquet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert a sqlite file to parquet format — sqlite_to_parquet","text":"path_to_file String indicates path input file (forget extension). table_in_sqlite string indicates name table convert sqlite file path_to_parquet String indicates path directory parquet files stored. partition String (\"yes\" \"\" - default) indicates whether want create partitioned parquet file. \"yes\", `\"partitioning\"` argument must filled . case, folder created modality variable filled `\"partitioning\"`. careful, argument can \"yes\" `max_memory` `max_rows` argument NULL. compression compression algorithm. Default \"snappy\". compression_level compression level. Meaning depends compression algorithm. ... additional format-specific arguments, see arrow::write_parquet() arrow::write_dataset() informations.","code":""},{"path":"/reference/sqlite_to_parquet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert a sqlite file to parquet format — sqlite_to_parquet","text":"parquet file, invisibly","code":""},{"path":"/reference/sqlite_to_parquet.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert a sqlite file to parquet format — sqlite_to_parquet","text":"","code":"# Conversion from a local sqlite file to a single parquet file :  sqlite_to_parquet(   path_to_file = system.file(\"extdata\",\"iris.sqlite\",package = \"parquetize\"),   table_in_sqlite = \"iris\",   path_to_parquet = tempfile(fileext = \".parquet\") ) #> Reading data... #> Writing data... #> Writing data... #> ✔ Data are available in parquet file under /tmp/RtmpBXcUtf/file15106aa43e74.parquet #> Writing data... #> Writing data...  # Conversion from a local sqlite file to a partitioned parquet file  :  sqlite_to_parquet(   path_to_file = system.file(\"extdata\",\"iris.sqlite\",package = \"parquetize\"),   table_in_sqlite = \"iris\",   path_to_parquet = tempfile(),   partition = \"yes\",   partitioning =  c(\"Species\") ) #> Reading data... #> Writing data... #> Writing data... #> ✔ Data are available in parquet dataset under /tmp/RtmpBXcUtf/file1510355b4fd7 #> Writing data... #> Writing data..."},{"path":"/reference/table_to_parquet.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert an input file to parquet format — table_to_parquet","title":"Convert an input file to parquet format — table_to_parquet","text":"function allows convert input file parquet format. handles SAS, SPSS Stata files function. one function use 3 cases. 3 cases, function guesses data format using extension input file (`path_to_file` argument). Two conversions possibilities offered : Convert single parquet file. Argument `path_to_parquet` must used; Convert partitioned parquet file. Additionnal arguments `partition` `partitioning` must used; avoid overcharging R's RAM, conversion can done chunk. One arguments `max_memory` `max_rows` must used. useful huge tables computers little RAM conversion done less memory consumption. information, see [](https://ddotta.github.io/parquetize/articles/aa-conversions.html).","code":""},{"path":"/reference/table_to_parquet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert an input file to parquet format — table_to_parquet","text":"","code":"table_to_parquet(   path_to_file,   path_to_parquet,   max_memory = NULL,   max_rows = NULL,   chunk_size = lifecycle::deprecated(),   chunk_memory_size = lifecycle::deprecated(),   columns = \"all\",   by_chunk = lifecycle::deprecated(),   skip = 0,   partition = \"no\",   encoding = NULL,   chunk_memory_sample_lines = 10000,   compression = \"snappy\",   compression_level = NULL,   ... )"},{"path":"/reference/table_to_parquet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert an input file to parquet format — table_to_parquet","text":"path_to_file String indicates path input file (forget extension). path_to_parquet String indicates path directory parquet files stored. max_memory Memory size (Mb) data one parquet file roughly fit. max_rows Number lines defines size chunk. argument can filled max_memory used. chunk_size DEPRECATED use max_rows chunk_memory_size DEPRECATED use max_memory columns Character vector columns select input file (default, columns selected). by_chunk DEPRECATED use max_memory max_rows instead skip default 0. argument must filled `by_chunk` TRUE. Number lines ignore converting. partition String (\"yes\" \"\" - default) indicates whether want create partitioned parquet file. \"yes\", `\"partitioning\"` argument must filled . case, folder created modality variable filled `\"partitioning\"`. careful, argument can \"yes\" `max_memory` `max_rows` argument NULL. encoding String indicates character encoding input file. chunk_memory_sample_lines Number lines read evaluate max_memory. Default 10 000. compression compression algorithm. Default \"snappy\". compression_level compression level. Meaning depends compression algorithm. ... Additional format-specific arguments,  see arrow::write_parquet() arrow::write_dataset() informations.","code":""},{"path":"/reference/table_to_parquet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert an input file to parquet format — table_to_parquet","text":"Parquet files, invisibly","code":""},{"path":"/reference/table_to_parquet.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert an input file to parquet format — table_to_parquet","text":"","code":"# Conversion from a SAS file to a single parquet file :  table_to_parquet(   path_to_file = system.file(\"examples\",\"iris.sas7bdat\", package = \"haven\"),   path_to_parquet = tempfile(fileext = \".parquet\") ) #> Reading data... #> Writing data... #> ✔ Data are available in parquet file under /tmp/RtmpBXcUtf/file1510450b420c.parquet #> Writing data... #> Reading data... #> ✔ The /home/runner/work/_temp/Library/haven/examples/iris.sas7bdat file is available in parquet format under /tmp/RtmpBXcUtf/file1510450b420c.parquet #> Reading data...  # Conversion from a SPSS file to a single parquet file :  table_to_parquet(   path_to_file = system.file(\"examples\",\"iris.sav\", package = \"haven\"),   path_to_parquet = tempfile(fileext = \".parquet\"), ) #> Reading data... #> Writing data... #> ✔ Data are available in parquet file under /tmp/RtmpBXcUtf/file15105508818f.parquet #> Writing data... #> Reading data... #> ✔ The /home/runner/work/_temp/Library/haven/examples/iris.sav file is available in parquet format under /tmp/RtmpBXcUtf/file15105508818f.parquet #> Reading data... # Conversion from a Stata file to a single parquet file without progress bar :  table_to_parquet(   path_to_file = system.file(\"examples\",\"iris.dta\", package = \"haven\"),   path_to_parquet = tempfile(fileext = \".parquet\") ) #> Reading data... #> Writing data... #> ✔ Data are available in parquet file under /tmp/RtmpBXcUtf/file15105595840c.parquet #> Writing data... #> Reading data... #> ✔ The /home/runner/work/_temp/Library/haven/examples/iris.dta file is available in parquet format under /tmp/RtmpBXcUtf/file15105595840c.parquet #> Reading data...  # Reading SPSS file by chunk (using `max_rows` argument) # and conversion to multiple parquet files :  table_to_parquet(   path_to_file = system.file(\"examples\",\"iris.sav\", package = \"haven\"),   path_to_parquet = tempfile(),   max_rows = 50, ) #> Reading data... #> Writing file15106f2c3193-1-50.parquet... #> Reading data... #> Writing file15106f2c3193-51-100.parquet... #> Reading data... #> Writing file15106f2c3193-101-150.parquet... #> Reading data... #> ✔ Data are available in parquet dataset under /tmp/RtmpBXcUtf/file15106f2c3193/ #> Reading data...  # Reading SPSS file by chunk (using `max_memory` argument) # and conversion to multiple parquet files of 5 Kb when loaded (5 Mb / 1024) # (in real files, you should use bigger value that fit in memory like 3000 # or 4000) :  table_to_parquet(   path_to_file = system.file(\"examples\",\"iris.sav\", package = \"haven\"),   path_to_parquet = tempfile(),   max_memory = 5 / 1024, ) #> Reading data... #> Writing file15104b4aec2d-1-82.parquet... #> Reading data... #> Writing file15104b4aec2d-83-150.parquet... #> ✔ Data are available in parquet dataset under /tmp/RtmpBXcUtf/file15104b4aec2d/ #> Writing file15104b4aec2d-83-150.parquet...  # Reading SAS file by chunk of 50 lines with encoding # and conversion to multiple files :  table_to_parquet(   path_to_file = system.file(\"examples\",\"iris.sas7bdat\", package = \"haven\"),   path_to_parquet = tempfile(),   max_rows = 50,   encoding = \"utf-8\" ) #> Reading data... #> Writing file1510144bb3e4-1-50.parquet... #> Reading data... #> Writing file1510144bb3e4-51-100.parquet... #> Reading data... #> Writing file1510144bb3e4-101-150.parquet... #> Reading data... #> ✔ Data are available in parquet dataset under /tmp/RtmpBXcUtf/file1510144bb3e4/ #> Reading data...  # Conversion from a SAS file to a single parquet file and select only # few columns  :  table_to_parquet(   path_to_file = system.file(\"examples\",\"iris.sas7bdat\", package = \"haven\"),   path_to_parquet = tempfile(fileext = \".parquet\"),   columns = c(\"Species\",\"Petal_Length\") ) #> Reading data... #> Writing data... #> ✔ Data are available in parquet file under /tmp/RtmpBXcUtf/file151039870b45.parquet #> Writing data... #> Reading data... #> ✔ The /home/runner/work/_temp/Library/haven/examples/iris.sas7bdat file is available in parquet format under /tmp/RtmpBXcUtf/file151039870b45.parquet #> Reading data...  # Conversion from a SAS file to a partitioned parquet file  :  table_to_parquet(   path_to_file = system.file(\"examples\",\"iris.sas7bdat\", package = \"haven\"),   path_to_parquet = tempfile(),   partition = \"yes\",   partitioning =  c(\"Species\") # vector use as partition key ) #> Reading data... #> Writing data... #> ✔ Data are available in parquet dataset under /tmp/RtmpBXcUtf/file151037d2e2fd #> Writing data... #> Reading data... #> ✔ The /home/runner/work/_temp/Library/haven/examples/iris.sas7bdat file is available in parquet format under /tmp/RtmpBXcUtf/file151037d2e2fd #> Reading data...  # Reading SAS file by chunk of 50 lines # and conversion to multiple files with zstd, compression level 10  if (isTRUE(arrow::arrow_info()$capabilities[['zstd']])) {   table_to_parquet(     path_to_file = system.file(\"examples\",\"iris.sas7bdat\", package = \"haven\"),     path_to_parquet = tempfile(),     max_rows = 50,     compression = \"zstd\",     compression_level = 10   ) } #> Reading data... #> Writing file15107da79c8d-1-50.parquet... #> Reading data... #> Writing file15107da79c8d-51-100.parquet... #> Reading data... #> Writing file15107da79c8d-101-150.parquet... #> Reading data... #> ✔ Data are available in parquet dataset under /tmp/RtmpBXcUtf/file15107da79c8d/ #> Reading data..."},{"path":"/reference/write_parquet_at_once.html","id":null,"dir":"Reference","previous_headings":"","what":"write parquet file or dataset based on partition argument  — write_parquet_at_once","title":"write parquet file or dataset based on partition argument  — write_parquet_at_once","text":"Low level function implements logic write parquet file dataset data","code":""},{"path":"/reference/write_parquet_at_once.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"write parquet file or dataset based on partition argument  — write_parquet_at_once","text":"","code":"write_parquet_at_once(   data,   path_to_parquet,   partition = \"no\",   compression = \"snappy\",   compression_level = NULL,   ... )"},{"path":"/reference/write_parquet_at_once.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"write parquet file or dataset based on partition argument  — write_parquet_at_once","text":"data data.frame/tibble write path_to_parquet String indicates path directory output parquet file dataset stored. partition string (\"yes\" \"\" - default) indicates whether want create partitioned parquet file. \"yes\", `\"partitioning\"` argument must filled . case, folder created modality variable filled `\"partitioning\"`. compression compression algorithm. Default \"snappy\". compression_level compression level. Meaning depends compression algorithm. ... Additional format-specific arguments,  see arrow::write_parquet()","code":""},{"path":"/reference/write_parquet_at_once.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"write parquet file or dataset based on partition argument  — write_parquet_at_once","text":"dataset return arrow::open_dataset","code":""},{"path":"/reference/write_parquet_at_once.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"write parquet file or dataset based on partition argument  — write_parquet_at_once","text":"","code":"write_parquet_at_once(iris, tempfile()) #> Writing data... #> ✔ Data are available in parquet file under /tmp/RtmpBXcUtf/file151038b362dd #> Writing data...  write_parquet_at_once(iris, tempfile(), compression=\"gzip\", compression_level = 5) #> Writing data... #> ✔ Data are available in parquet file under /tmp/RtmpBXcUtf/file1510b2e6215 #> Writing data...  write_parquet_at_once(iris, tempfile(), partition = \"yes\", partitioning = c(\"Species\")) #> Writing data... #> ✔ Data are available in parquet dataset under /tmp/RtmpBXcUtf/file15104fe55747 #> Writing data..."},{"path":"/reference/write_parquet_by_chunk.html","id":null,"dir":"Reference","previous_headings":"","what":"read input by chunk on function and create dataset  — write_parquet_by_chunk","title":"read input by chunk on function and create dataset  — write_parquet_by_chunk","text":"Low level function implements logic read input file chunk write   dataset. : calculate number row chunk needed; loop input file chunk; write output files.","code":""},{"path":"/reference/write_parquet_by_chunk.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"read input by chunk on function and create dataset  — write_parquet_by_chunk","text":"","code":"write_parquet_by_chunk(   read_method,   input,   path_to_parquet,   max_rows = NULL,   max_memory = NULL,   chunk_memory_sample_lines = 10000,   compression = \"snappy\",   compression_level = NULL,   ... )"},{"path":"/reference/write_parquet_by_chunk.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"read input by chunk on function and create dataset  — write_parquet_by_chunk","text":"read_method method read input files. method take three   arguments `input` : kind data. Can   `skip` : number row skip   `n_max` : number row return method called returns dataframe/tibble zero row. need pass argument, can use   [closure](http://adv-r..co.nz/Functional-programming.html#closures). See   last example. input indicates path input. can anything want often file's path data.frame. path_to_parquet String indicates path directory output parquet file dataset stored. max_rows Number lines defines size chunk. argument can filled max_memory used. max_memory Memory size (Mb) data one parquet file roughly fit. chunk_memory_sample_lines Number lines read evaluate max_memory. Default 10 000. compression compression algorithm. Default \"snappy\". compression_level compression level. Meaning depends compression algorithm. ... Additional format-specific arguments,  see arrow::write_parquet()","code":""},{"path":"/reference/write_parquet_by_chunk.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"read input by chunk on function and create dataset  — write_parquet_by_chunk","text":"dataset return arrow::open_dataset","code":""},{"path":"/reference/write_parquet_by_chunk.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"read input by chunk on function and create dataset  — write_parquet_by_chunk","text":"","code":"# example with a dataframe  # we create the function to loop over the data.frame  read_method <- function(input, skip = 0L, n_max = Inf) {   # if we are after the end of the input we return an empty data.frame   if (skip+1 > nrow(input)) { return(data.frame()) }    # return the n_max row from skip + 1   input[(skip+1):(min(skip+n_max, nrow(input))),] }  # we use it  write_parquet_by_chunk(   read_method = read_method,   input = mtcars,   path_to_parquet = tempfile(),   max_rows = 10, ) #> Reading data... #> Writing file1510662f52b-1-10.parquet... #> Reading data... #> Writing file1510662f52b-11-20.parquet... #> Reading data... #> Writing file1510662f52b-21-30.parquet... #> Reading data... #> Writing file1510662f52b-31-32.parquet... #> ✔ Data are available in parquet dataset under /tmp/RtmpBXcUtf/file1510662f52b/ #> Writing file1510662f52b-31-32.parquet...   # # Example with haven::read_sas #  # we need to pass two argument beside the 3 input, skip and n_max. # We will use a closure :  my_read_closure <- function(encoding, columns) {   function(input, skip = OL, n_max = Inf) {     haven::read_sas(data_file = input,                     n_max = n_max,                     skip = skip,                     encoding = encoding,                     col_select = all_of(columns))   } }  # we initialize the closure  read_method <- my_read_closure(encoding = \"WINDOWS-1252\", columns = c(\"Species\", \"Petal_Width\"))  # we use it write_parquet_by_chunk(   read_method = read_method,   input = system.file(\"examples\",\"iris.sas7bdat\", package = \"haven\"),   path_to_parquet = tempfile(),   max_rows = 75, ) #> Reading data... #> Writing file151050caaeab-1-75.parquet... #> Reading data... #> Writing file151050caaeab-76-150.parquet... #> Reading data... #> ✔ Data are available in parquet dataset under /tmp/RtmpBXcUtf/file151050caaeab/ #> Reading data..."},{"path":"/news/index.html","id":"parquetize-056","dir":"Changelog","previous_headings":"","what":"parquetize 0.5.6","title":"parquetize 0.5.6","text":"release includes :","code":""},{"path":"/news/index.html","id":"possibility-to-use-a-rdbms-as-source-0-5-6","dir":"Changelog","previous_headings":"","what":"Possibility to use a RDBMS as source","title":"parquetize 0.5.6","text":"can convert parquet query want DBI compatible RDBMS : can find information dbi_to_parquet documentation.","code":"dbi_connection <- DBI::dbConnect(RSQLite::SQLite(),   system.file(\"extdata\",\"iris.sqlite\",package = \"parquetize\"))    # Reading iris table from local sqlite database # and conversion to one parquet file : dbi_to_parquet(   conn = dbi_connection,   sql_query = \"SELECT * FROM iris\",   path_to_parquet = tempdir(),   parquetname = \"iris\" )"},{"path":"/news/index.html","id":"check_parquet-function-0-5-6","dir":"Changelog","previous_headings":"","what":"check_parquet function","title":"parquetize 0.5.6","text":"new check_parquet function check dataset/file valid return columns arrow type","code":""},{"path":"/news/index.html","id":"deprecations-0-5-6","dir":"Changelog","previous_headings":"","what":"Deprecations","title":"parquetize 0.5.6","text":"Two arguments deprecated avoid confusion arrow concept keep consistency chunk_size replaced max_rows (chunk size arrow concept). chunk_memory_size replaced max_memory consistency","code":""},{"path":"/news/index.html","id":"other-0-5-6","dir":"Changelog","previous_headings":"","what":"Other","title":"parquetize 0.5.6","text":"refactoring : extract logic write parquet files chunk write_parquet_by_chunk write_parquet_at_once big test’s refactoring : _to_parquet output files formally validate (readable parquet, number lines, partitions, number files). use cli_abort instead cli_alert_danger stop(““) everywhere minors changes bugfix: table_to_parquet select columns expected bugfix: skip_if_offline tests download","code":""},{"path":"/news/index.html","id":"parquetize-055","dir":"Changelog","previous_headings":"","what":"parquetize 0.5.5","title":"parquetize 0.5.5","text":"CRAN release: 2023-03-28 release includes :","code":""},{"path":"/news/index.html","id":"a-very-important-new-contributor-to-parquetize--0-5-5","dir":"Changelog","previous_headings":"","what":"A very important new contributor to parquetize !","title":"parquetize 0.5.5","text":"Due numerous contributions, @nbc now officially part project authors !","code":""},{"path":"/news/index.html","id":"three-arguments-deprecation-0-5-5","dir":"Changelog","previous_headings":"","what":"Three arguments deprecation","title":"parquetize 0.5.5","text":"big refactoring, three arguments deprecated : by_chunk : table_to_parquet automatically chunked use one chunk_memory_size chunk_size. csv_as_a_zip: csv_to_table detect file zip extension. url_to_csv : use path_to_csv instead, csv_to_table detect file remote file path. raise deprecation warning moment.","code":""},{"path":"/news/index.html","id":"chunking-by-memory-size-0-5-5","dir":"Changelog","previous_headings":"","what":"Chunking by memory size","title":"parquetize 0.5.5","text":"possibility chunk parquet memory size table_to_parquet(): table_to_parquet() takes chunk_memory_size argument convert input file parquet file roughly chunk_memory_size Mb size data loaded memory. Argument by_chunk deprecated (see ). Example use argument chunk_memory_size:","code":"table_to_parquet(   path_to_table = system.file(\"examples\",\"iris.sas7bdat\", package = \"haven\"),   path_to_parquet = tempdir(),   chunk_memory_size = 5000, # this will create files of around 5Gb when loaded in memory )"},{"path":"/news/index.html","id":"passing-argument-like-compression-to-write_parquet-when-chunking-0-5-5","dir":"Changelog","previous_headings":"","what":"Passing argument like compression to write_parquet when chunking","title":"parquetize 0.5.5","text":"functionality users pass argument write_parquet() chunking argument (ellipsis). Can used example pass compression compression_level. Example:","code":"table_to_parquet(   path_to_table = system.file(\"examples\",\"iris.sas7bdat\", package = \"haven\"),   path_to_parquet = tempdir(),   compression = \"zstd\",   compression_level = 10,   chunk_memory_size = 5000 )"},{"path":"/news/index.html","id":"a-new-function-download_extract-0-5-5","dir":"Changelog","previous_headings":"","what":"A new function download_extract","title":"parquetize 0.5.5","text":"function added … download unzip file needed.","code":"file_path <- download_extract(   \"https://www.nomisweb.co.uk/output/census/2021/census2021-ts007.zip\",   filename_in_zip = \"census2021-ts007-ctry.csv\" ) csv_to_parquet(   file_path,   path_to_parquet = tempdir() )"},{"path":"/news/index.html","id":"other-0-5-5","dir":"Changelog","previous_headings":"","what":"Other","title":"parquetize 0.5.5","text":"cover, release hardened tests","code":""},{"path":"/news/index.html","id":"parquetize-054","dir":"Changelog","previous_headings":"","what":"parquetize 0.5.4","title":"parquetize 0.5.4","text":"CRAN release: 2023-03-13 release fix error converting sas file chunk.","code":""},{"path":"/news/index.html","id":"parquetize-053","dir":"Changelog","previous_headings":"","what":"parquetize 0.5.3","title":"parquetize 0.5.3","text":"CRAN release: 2023-02-20 release includes : Added columns selection table_to_parquet() csv_to_parquet() functions #20 example files parquet format iris table migrated inst/extdata directory.","code":""},{"path":"/news/index.html","id":"parquetize-052","dir":"Changelog","previous_headings":"","what":"parquetize 0.5.2","title":"parquetize 0.5.2","text":"release includes : behaviour table_to_parquet() function fixed argument by_chunk TRUE.","code":""},{"path":"/news/index.html","id":"parquetize-051","dir":"Changelog","previous_headings":"","what":"parquetize 0.5.1","title":"parquetize 0.5.1","text":"CRAN release: 2023-01-30 release removes duckdb_to_parquet() function advice Brian Ripley CRAN. Indeed, storage DuckDB yet stable. storage stabilized version 1.0 releases.","code":""},{"path":"/news/index.html","id":"parquetize-050","dir":"Changelog","previous_headings":"","what":"parquetize 0.5.0","title":"parquetize 0.5.0","text":"CRAN release: 2023-01-13 release includes corrections CRAN submission.","code":""},{"path":"/news/index.html","id":"parquetize-040","dir":"Changelog","previous_headings":"","what":"parquetize 0.4.0","title":"parquetize 0.4.0","text":"release includes important feature : table_to_parquet() function can now convert tables parquet format less memory consumption. Useful huge tables computers little RAM. (#15) vignette written . See . Removal nb_rows argument table_to_parquet() function Replaced new arguments by_chunk, chunk_size skip (see documentation) Progress bars now managed {cli} package","code":""},{"path":"/news/index.html","id":"parquetize-030","dir":"Changelog","previous_headings":"","what":"parquetize 0.3.0","title":"parquetize 0.3.0","text":"Added duckdb_to_parquet() function convert duckdb files parquet format. Added sqlite_to_parquet() function convert sqlite files parquet format.","code":""},{"path":"/news/index.html","id":"parquetize-020","dir":"Changelog","previous_headings":"","what":"parquetize 0.2.0","title":"parquetize 0.2.0","text":"Added rds_to_parquet() function convert rds files parquet format. Added json_to_parquet() function convert json ndjson files parquet format. Added possibility convert csv file partitioned parquet file. Improving code coverage (#9) Check path_to_parquet exists functions csv_to_parquet() table_to_parquet() (@py-b)","code":""},{"path":"/news/index.html","id":"parquetize-010","dir":"Changelog","previous_headings":"","what":"parquetize 0.1.0","title":"parquetize 0.1.0","text":"Added table_to_parquet() function convert SAS, SPSS Stata files parquet format. Added csv_to_parquet() function convert csv files parquet format. Added parquetize_example() function get path package data examples. Added NEWS.md file track changes package.","code":""}]
